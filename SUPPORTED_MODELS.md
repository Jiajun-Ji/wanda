# Wanda æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨

æœ¬æ–‡æ¡£åˆ—å‡ºäº† Wanda å‰ªææ¡†æ¶æ”¯æŒçš„æ‰€æœ‰ LLM æ¨¡å‹ã€‚

---

## ğŸ“‹ æ”¯æŒçš„æ¨¡å‹æ¶æ„

Wanda ä½¿ç”¨ Hugging Face çš„ `AutoModelForCausalLM`ï¼Œç†è®ºä¸Šæ”¯æŒæ‰€æœ‰ Causal LM æ¶æ„ã€‚ä»¥ä¸‹æ˜¯ç»è¿‡æµ‹è¯•å’ŒéªŒè¯çš„æ¨¡å‹ï¼š

### âœ… å®˜æ–¹æ”¯æŒï¼ˆå·²æµ‹è¯•ï¼‰

| æ¨¡å‹ç³»åˆ— | æ¨¡å‹å¤§å° | Hugging Face æ¨¡å‹ ID | è„šæœ¬ |
|---------|---------|---------------------|------|
| **LLaMA-1** | 7B | `decapoda-research/llama-7b-hf` | `scripts/llama_7b.sh` |
| **LLaMA-1** | 13B | `decapoda-research/llama-13b-hf` | `scripts/llama_13b.sh` |
| **LLaMA-1** | 30B | `decapoda-research/llama-30b-hf` | `scripts/llama_30b.sh` |
| **LLaMA-1** | 65B | `decapoda-research/llama-65b-hf` | `scripts/llama_65b.sh` |
| **LLaMA-2** | 7B | `meta-llama/Llama-2-7b-hf` | âœ… |
| **LLaMA-2** | 13B | `meta-llama/Llama-2-13b-hf` | âœ… |
| **LLaMA-2** | 70B | `meta-llama/Llama-2-70b-hf` | âœ… |
| **OPT** | 125M - 66B | `facebook/opt-*` | `main_opt.py` |

### ğŸ”§ ç†è®ºæ”¯æŒï¼ˆæœªå®˜æ–¹æµ‹è¯•ï¼Œä½†åº”è¯¥å¯ç”¨ï¼‰

åŸºäº `AutoModelForCausalLM` çš„ä»»ä½•æ¨¡å‹éƒ½åº”è¯¥å¯ä»¥å·¥ä½œï¼ŒåŒ…æ‹¬ï¼š

| æ¨¡å‹ç³»åˆ— | ç¤ºä¾‹æ¨¡å‹ ID | è¯´æ˜ |
|---------|------------|------|
| **LLaMA-3** | `meta-llama/Meta-Llama-3-8B` | æœ€æ–°çš„ LLaMA ç³»åˆ— |
| **Mistral** | `mistralai/Mistral-7B-v0.1` | Mistral AI çš„å¼€æºæ¨¡å‹ |
| **Mixtral** | `mistralai/Mixtral-8x7B-v0.1` | MoE æ¶æ„ |
| **Qwen** | `Qwen/Qwen-7B` | é˜¿é‡Œå·´å·´çš„é€šä¹‰åƒé—® |
| **Baichuan** | `baichuan-inc/Baichuan2-7B-Base` | ç™¾å·æ™ºèƒ½ |
| **Yi** | `01-ai/Yi-6B` | é›¶ä¸€ä¸‡ç‰© |
| **DeepSeek** | `deepseek-ai/deepseek-llm-7b-base` | DeepSeek |
| **Phi** | `microsoft/phi-2` | Microsoft å°æ¨¡å‹ |
| **Gemma** | `google/gemma-7b` | Google |
| **BLOOM** | `bigscience/bloom-*` | BigScience |
| **GPT-Neo/J** | `EleutherAI/gpt-neo-*` | EleutherAI |
| **Falcon** | `tiiuae/falcon-*` | TII |

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1ï¸âƒ£ LLaMA ç³»åˆ—

#### LLaMA-1
```bash
python main.py \
    --model decapoda-research/llama-7b-hf \
    --prune_method wanda \
    --sparsity_ratio 0.5 \
    --sparsity_type unstructured \
    --save out/llama_7b/unstructured/wanda/
```

#### LLaMA-2
```bash
python main.py \
    --model meta-llama/Llama-2-7b-hf \
    --prune_method wanda \
    --sparsity_ratio 0.5 \
    --sparsity_type unstructured \
    --save out/llama2_7b/unstructured/wanda/
```

#### LLaMA-3ï¼ˆç†è®ºæ”¯æŒï¼‰
```bash
python main.py \
    --model meta-llama/Meta-Llama-3-8B \
    --prune_method wanda \
    --sparsity_ratio 0.5 \
    --sparsity_type unstructured \
    --save out/llama3_8b/unstructured/wanda/
```

### 2ï¸âƒ£ OPT ç³»åˆ—

ä½¿ç”¨ä¸“é—¨çš„ `main_opt.py` è„šæœ¬ï¼š

```bash
python main_opt.py \
    --model facebook/opt-6.7b \
    --prune_method wanda \
    --sparsity_ratio 0.5 \
    --sparsity_type unstructured \
    --save out/opt_6.7b/unstructured/wanda/
```

**å¯ç”¨çš„ OPT æ¨¡å‹**ï¼š
- `facebook/opt-125m`
- `facebook/opt-350m`
- `facebook/opt-1.3b`
- `facebook/opt-2.7b`
- `facebook/opt-6.7b`
- `facebook/opt-13b`
- `facebook/opt-30b`
- `facebook/opt-66b`

### 3ï¸âƒ£ å…¶ä»–æ¨¡å‹ï¼ˆé€šç”¨æ–¹æ³•ï¼‰

å¯¹äºå…¶ä»–åŸºäº Transformer çš„ Causal LM æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨ `main.py`ï¼š

```bash
python main.py \
    --model <huggingface_model_id> \
    --prune_method wanda \
    --sparsity_ratio 0.5 \
    --sparsity_type unstructured \
    --save out/<model_name>/unstructured/wanda/
```

---

## ğŸ” æ¨¡å‹æ¶æ„è¦æ±‚

Wanda å¯¹æ¨¡å‹æ¶æ„æœ‰ä»¥ä¸‹è¦æ±‚ï¼š

### âœ… å¿…é¡»æ»¡è¶³

1. **Causal Language Model**: æ¨¡å‹å¿…é¡»æ˜¯è‡ªå›å½’çš„å› æœè¯­è¨€æ¨¡å‹
2. **Transformer æ¶æ„**: åŸºäº Transformer çš„æ¶æ„
3. **çº¿æ€§å±‚**: åŒ…å«æ ‡å‡†çš„ `nn.Linear` å±‚ï¼ˆç”¨äºå‰ªæï¼‰
4. **Hugging Face å…¼å®¹**: å¯ä»¥é€šè¿‡ `AutoModelForCausalLM.from_pretrained()` åŠ è½½

### âš ï¸ æ¶æ„å·®å¼‚å¤„ç†

ä¸åŒæ¨¡å‹æ¶æ„çš„å±‚å‘½åå¯èƒ½ä¸åŒï¼š

| æ¨¡å‹ | å±‚è®¿é—®è·¯å¾„ | è¯´æ˜ |
|------|-----------|------|
| **LLaMA** | `model.layers` | æ ‡å‡† Transformer å±‚ |
| **OPT** | `model.decoder.layers` | Decoder-only æ¶æ„ |
| **BLOOM** | `transformer.h` | GPT é£æ ¼å‘½å |
| **GPT-Neo** | `transformer.h` | GPT é£æ ¼å‘½å |

**è§£å†³æ–¹æ¡ˆ**ï¼š
- LLaMA ç³»åˆ—ä½¿ç”¨ `main.py` å’Œ `lib/prune.py`
- OPT ç³»åˆ—ä½¿ç”¨ `main_opt.py` å’Œ `lib/prune_opt.py`
- å…¶ä»–æ¨¡å‹å¯èƒ½éœ€è¦ä¿®æ”¹å±‚è®¿é—®è·¯å¾„

---

## ğŸ§ª æµ‹è¯•æ–°æ¨¡å‹

å¦‚æœä½ æƒ³åœ¨æ–°æ¨¡å‹ä¸Šä½¿ç”¨ Wandaï¼ŒæŒ‰ä»¥ä¸‹æ­¥éª¤æµ‹è¯•ï¼š

### æ­¥éª¤ 1: æ£€æŸ¥æ¨¡å‹å…¼å®¹æ€§

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "your-model-id"
model = AutoModelForCausalLM.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)

# æ£€æŸ¥æ¨¡å‹ç»“æ„
print(model)

# æŸ¥æ‰¾ Transformer å±‚
# LLaMA: model.layers
# OPT: model.decoder.layers
# BLOOM: transformer.h
```

### æ­¥éª¤ 2: å°è§„æ¨¡æµ‹è¯•

```bash
python main.py \
    --model your-model-id \
    --prune_method wanda \
    --sparsity_ratio 0.1 \
    --sparsity_type unstructured \
    --nsamples 16 \
    --save out/test/
```

### æ­¥éª¤ 3: æ£€æŸ¥å‰ªæç»“æœ

```python
from lib.prune import check_sparsity

# åŠ è½½å‰ªæåçš„æ¨¡å‹
pruned_model = AutoModelForCausalLM.from_pretrained("out/test/")

# æ£€æŸ¥ç¨€ç–åº¦
sparsity = check_sparsity(pruned_model)
print(f"Sparsity: {sparsity}")
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†

### LLaMA-2 æ€§èƒ½ï¼ˆWikiText2 PPLï¼‰

| æ¨¡å‹ | Dense | Magnitude 50% | Wanda 50% | SparseGPT 50% |
|------|-------|---------------|-----------|---------------|
| LLaMA-2-7B | 5.12 | 14.89 | **6.29** | 6.15 |
| LLaMA-2-13B | 4.57 | 6.37 | **5.01** | 4.95 |
| LLaMA-2-70B | 3.12 | 4.98 | **3.56** | 3.49 |

### LLaMA-1 æ€§èƒ½ï¼ˆWikiText2 PPLï¼‰

| æ¨¡å‹ | Dense | Magnitude 50% | Wanda 50% | SparseGPT 50% |
|------|-------|---------------|-----------|---------------|
| LLaMA-7B | 5.68 | 15.87 | **6.96** | 6.61 |
| LLaMA-13B | 5.09 | 7.75 | **5.59** | 5.50 |
| LLaMA-30B | 4.10 | 21.18 | **4.60** | 4.48 |
| LLaMA-65B | 3.53 | 4.48 | **3.80** | 3.73 |

---

## âš™ï¸ æ··åˆä¸‰å±‚å‰ªææ”¯æŒ

ä½ çš„é¡¹ç›®æ‰©å±•äº† Wandaï¼Œæ”¯æŒæ··åˆä¸‰å±‚å‰ªææ¨¡å¼ï¼š

### æ”¯æŒçš„æ¨¡å‹

æ‰€æœ‰æ”¯æŒæ ‡å‡† Wanda çš„æ¨¡å‹éƒ½æ”¯æŒæ··åˆä¸‰å±‚å‰ªæï¼š

```bash
python main_block_three_tier.py \
    --model meta-llama/Llama-2-7b-hf \
    --sparsity_ratios 0.35 0.45 0.2 \
    --save out/llama2_7b/block_16x16_three_tier/
```

**ä¸‰å±‚æ¨¡å¼**ï¼š
1. **Tier 1 (Dense)**: 35% çš„å—ä¿æŒå¯†é›†
2. **Tier 2 (2:4 Sparse)**: 45% çš„å—ä½¿ç”¨ 2:4 ç¨€ç–
3. **Tier 3 (Top-K)**: 20% çš„å—ä½¿ç”¨æåº¦ç¨€ç–ï¼ˆTop-Kï¼‰

---

## ğŸ”§ æ·»åŠ æ–°æ¨¡å‹æ”¯æŒ

å¦‚æœä½ çš„æ¨¡å‹ä¸åœ¨æ”¯æŒåˆ—è¡¨ä¸­ï¼Œå¯ä»¥æŒ‰ä»¥ä¸‹æ­¥éª¤æ·»åŠ ï¼š

### 1. ç¡®å®šå±‚è®¿é—®è·¯å¾„

```python
import torch
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("your-model")

# æ‰“å°æ¨¡å‹ç»“æ„
print(model)

# å¸¸è§è·¯å¾„ï¼š
# - model.layers (LLaMA, Mistral, Qwen)
# - model.decoder.layers (OPT)
# - transformer.h (BLOOM, GPT-Neo)
# - model.model.layers (æŸäº›æ¨¡å‹)
```

### 2. ä¿®æ”¹ `lib/prune.py`

å¦‚æœå±‚è·¯å¾„ä¸æ˜¯ `model.layers`ï¼Œéœ€è¦ä¿®æ”¹ï¼š

```python
# åœ¨ prune_wanda() å‡½æ•°ä¸­
# åŸä»£ç ï¼š
layers = model.model.layers

# ä¿®æ”¹ä¸ºï¼š
layers = model.model.decoder.layers  # å¯¹äº OPT
# æˆ–
layers = model.transformer.h  # å¯¹äº BLOOM/GPT-Neo
```

### 3. æµ‹è¯•å¹¶æäº¤

æµ‹è¯•æˆåŠŸåï¼Œæ¬¢è¿æäº¤ PR æˆ– Issueï¼

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [Wanda è®ºæ–‡](https://arxiv.org/abs/2306.11695)
- [Wanda GitHub](https://github.com/locuslab/wanda)
- [Hugging Face Models](https://huggingface.co/models)
- [Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers)

---

## ğŸ’¡ å¸¸è§é—®é¢˜

### Q1: æˆ‘çš„æ¨¡å‹ä¸åœ¨åˆ—è¡¨ä¸­ï¼Œèƒ½ç”¨ Wanda å—ï¼Ÿ

**A**: å¦‚æœä½ çš„æ¨¡å‹æ˜¯åŸºäº Transformer çš„ Causal LMï¼Œå¾ˆå¯èƒ½å¯ä»¥ä½¿ç”¨ã€‚å…ˆç”¨å°è§„æ¨¡æµ‹è¯•ï¼ˆ`--sparsity_ratio 0.1 --nsamples 16`ï¼‰éªŒè¯ã€‚

### Q2: OPT å’Œ LLaMA æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**A**: ä¸»è¦æ˜¯å±‚è®¿é—®è·¯å¾„ä¸åŒï¼š
- LLaMA: `model.layers`
- OPT: `model.decoder.layers`

å› æ­¤ OPT éœ€è¦ä½¿ç”¨ä¸“é—¨çš„ `main_opt.py`ã€‚

### Q3: æ”¯æŒ Encoder-Decoder æ¨¡å‹ï¼ˆå¦‚ T5ï¼‰å—ï¼Ÿ

**A**: ä¸æ”¯æŒã€‚Wanda ä¸“é—¨ä¸º Decoder-only çš„ Causal LM è®¾è®¡ã€‚

### Q4: æ”¯æŒé‡åŒ–æ¨¡å‹å—ï¼Ÿ

**A**: ç†è®ºä¸Šæ”¯æŒï¼Œä½†éœ€è¦ç¡®ä¿æ¨¡å‹å¯ä»¥åŠ è½½ä¸º `float16`ã€‚é‡åŒ–æ¨¡å‹ï¼ˆå¦‚ GPTQã€AWQï¼‰å¯èƒ½éœ€è¦é¢å¤–å¤„ç†ã€‚

### Q5: å¤š GPU æ”¯æŒå—ï¼Ÿ

**A**: æ”¯æŒï¼ä½¿ç”¨ `device_map="auto"` è‡ªåŠ¨åˆ†é…ã€‚å¯¹äº 30B+ æ¨¡å‹ï¼Œä»£ç ä¼šè‡ªåŠ¨ä½¿ç”¨å¤š GPUã€‚

---

**æœ€åæ›´æ–°**: 2025-01-XX  
**ç»´æŠ¤è€…**: Jiajun Ji  
**é¡¹ç›®**: Wanda Hybrid Pruning

