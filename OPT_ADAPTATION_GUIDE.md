# OPT æ¨¡å‹é€‚é…æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•å°† Progressive Three-Tier Pruning é€‚é…åˆ° OPT æ¨¡å‹ï¼Œå¹¶æ”¯æŒä¸åŒçš„æ ¡å‡†/å¾®è°ƒæ•°æ®é›†ï¼ˆWikiText2 å’Œ C4ï¼‰ã€‚

---

## ğŸ” å…³é”®æ¶æ„å·®å¼‚

### Llama vs OPT

| ç»„ä»¶ | Llama | OPT |
|------|-------|-----|
| **å±‚è®¿é—®è·¯å¾„** | `model.model.layers` | `model.model.decoder.layers` |
| **å±‚æ•°é‡è·å–** | `len(model.model.layers)` | `len(model.model.decoder.layers)` |
| **Embedding** | `model.embed_tokens` | `model.embed_tokens` |
| **Position IDs** | âœ… éœ€è¦ï¼ˆRoPEï¼‰ | âŒ ä¸éœ€è¦ï¼ˆLearned PEï¼‰ |
| **Device Map Key** | `model.layers.{i}` | `model.decoder.layers.{i}` |

---

## ğŸ“ éœ€è¦ä¿®æ”¹/åˆ›å»ºçš„æ–‡ä»¶

### 1. âœ… å·²åˆ›å»ºçš„æ–‡ä»¶

#### `run_progressive_three_tier_universal.sh`
- **åŠŸèƒ½**ï¼šç»Ÿä¸€çš„å¯åŠ¨è„šæœ¬ï¼Œæ”¯æŒ Llama å’Œ OPT
- **é…ç½®é¡¹**ï¼š
  ```bash
  MODEL_TYPE="opt"  # æˆ– "llama"
  DATASET="wikitext2"  # æˆ– "c4"
  BASE_MODEL="/path/to/opt-model"
  ```

#### `main_progressive_three_tier_opt.py`
- **åŠŸèƒ½**ï¼šOPT ç‰ˆæœ¬çš„ progressive pruning ä¸»è„šæœ¬
- **å¯¼å…¥**ï¼šä½¿ç”¨ `lib.prune_opt` è€Œä¸æ˜¯ `lib.prune`

---

### 2. âš ï¸ éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶

#### `lib/prune_opt.py`
**éœ€è¦æ·»åŠ çš„å‡½æ•°**ï¼š

```python
def prune_wanda_progressive_three_tier_opt(
    args, model, tokenizer, device,
    block_size,
    target_dense_ratio,
    target_2_4_ratio,
    target_topk_ratio,
    dense_to_2_4_ratio,
    mid_2_4_to_topk_ratio,
    topk_per_block,
    previous_tier_maps=None
):
    """
    Progressive three-tier pruning for OPT models.
    
    Key differences from Llama version:
    1. Use model.model.decoder.layers instead of model.model.layers
    2. No position_ids in forward pass
    3. Different device map keys
    """
    # Implementation similar to prune_wanda_progressive_three_tier in lib/prune.py
    # but adapted for OPT architecture
```

**éœ€è¦æ·»åŠ çš„è¾…åŠ©å‡½æ•°**ï¼š

```python
def save_tier_map(tier_maps, filepath):
    """Save tier maps to file."""
    torch.save({
        'tier_map': tier_maps,
    }, filepath)

def load_tier_map(filepath):
    """Load tier maps from file."""
    data = torch.load(filepath)
    return data['tier_map']
```

---

## ğŸ”§ å…·ä½“ä¿®æ”¹æ­¥éª¤

### æ­¥éª¤ 1ï¼šåœ¨ `lib/prune_opt.py` ä¸­æ·»åŠ å‡½æ•°

éœ€è¦ä» `lib/prune.py` å¤åˆ¶ä»¥ä¸‹å‡½æ•°å¹¶ä¿®æ”¹ï¼š

1. **`prune_wanda_progressive_three_tier_opt()`**
   - å¤åˆ¶ `prune_wanda_progressive_three_tier()` çš„å®ç°
   - ä¿®æ”¹æ‰€æœ‰ `model.model.layers` â†’ `model.model.decoder.layers`
   - ç§»é™¤æ‰€æœ‰ `position_ids` ç›¸å…³ä»£ç 
   - ä¿®æ”¹ device map æ£€æŸ¥ï¼š`f"model.layers.{i}"` â†’ `f"model.decoder.layers.{i}"`

2. **`save_tier_map()` å’Œ `load_tier_map()`**
   - ç›´æ¥å¤åˆ¶å³å¯ï¼Œæ— éœ€ä¿®æ”¹

### æ­¥éª¤ 2ï¼šä¿®æ”¹å…³é”®ä»£ç æ®µ

#### åŸå§‹ä»£ç ï¼ˆLlamaï¼‰ï¼š
```python
# lib/prune.py
layers = model.model.layers

# Forward pass
outs[j] = layer(inps[j].unsqueeze(0), 
                attention_mask=attention_mask, 
                position_ids=position_ids)[0]

# Device map check
if f"model.layers.{i}" in model.hf_device_map:
    dev = model.hf_device_map[f"model.layers.{i}"]
```

#### ä¿®æ”¹åä»£ç ï¼ˆOPTï¼‰ï¼š
```python
# lib/prune_opt.py
layers = model.model.decoder.layers

# Forward pass (no position_ids)
outs[j] = layer(inps[j].unsqueeze(0), 
                attention_mask=attention_mask)[0]

# Device map check
if f"model.decoder.layers.{i}" in model.hf_device_map:
    dev = model.hf_device_map[f"model.decoder.layers.{i}"]
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹æ¡ˆ Aï¼šä½¿ç”¨ç»Ÿä¸€è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
cd /home/jjji/Research/Hybird-Kernel/wanda

# 1. ç¼–è¾‘ run_progressive_three_tier_universal.sh
# ä¿®æ”¹ä»¥ä¸‹é…ç½®ï¼š
MODEL_TYPE="opt"
BASE_MODEL="/mnt/sdb/llm_models/opt-1.3b"
DATASET="wikitext2"  # ç¬¬ä¸€æ¬¡è¿è¡Œ

# 2. è¿è¡Œ
chmod +x run_progressive_three_tier_universal.sh
./run_progressive_three_tier_universal.sh

# 3. ç¬¬äºŒæ¬¡è¿è¡Œï¼ˆä½¿ç”¨ C4ï¼‰
# ä¿®æ”¹é…ç½®ï¼š
DATASET="c4"
OUTPUT_BASE="out/progressive_three_tier_opt_c4"

# 4. å†æ¬¡è¿è¡Œ
./run_progressive_three_tier_universal.sh
```

### æ–¹æ¡ˆ Bï¼šæ‰‹åŠ¨è¿è¡Œæ¯ä¸ªæ­¥éª¤

```bash
# Iteration 1
CUDA_VISIBLE_DEVICES=0 python main_progressive_three_tier_opt.py \
    --model /mnt/sdb/llm_models/opt-1.3b \
    --iteration 1 \
    --config progressive_config.csv \
    --block_size 16 \
    --topk_per_block 15 \
    --save out/progressive_three_tier_opt_wikitext2/iter1/ \
    --save_model out/progressive_three_tier_opt_wikitext2/iter1/pruned_model

# Finetune
cd dense_ft
CUDA_VISIBLE_DEVICES=0,3 torchrun --nproc_per_node=2 finetune_sparse_model.py \
    --model_name_or_path ../out/progressive_three_tier_opt_wikitext2/iter1/pruned_model \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --num_train_epochs 2 \
    --learning_rate 2e-5 \
    --bf16 \
    --output_dir ../out/progressive_three_tier_opt_wikitext2/iter1/finetuned_model

# ... é‡å¤ iteration 2-5
```

---

## ğŸ“Š æ•°æ®é›†é…ç½®

### WikiText2
```bash
DATASET_NAME="wikitext"
DATASET_CONFIG="wikitext-2-raw-v1"
```

### C4
```bash
DATASET_NAME="allenai/c4"
DATASET_CONFIG="en"
```

**æ³¨æ„**ï¼šC4 æ•°æ®é›†è¾ƒå¤§ï¼Œé¦–æ¬¡åŠ è½½å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ã€‚

---

## âœ… éªŒè¯æ¸…å•

å®Œæˆä¿®æ”¹åï¼Œè¯·æ£€æŸ¥ï¼š

- [ ] `lib/prune_opt.py` ä¸­æ·»åŠ äº† `prune_wanda_progressive_three_tier_opt()`
- [ ] `lib/prune_opt.py` ä¸­æ·»åŠ äº† `save_tier_map()` å’Œ `load_tier_map()`
- [ ] æ‰€æœ‰ `model.model.layers` æ”¹ä¸º `model.model.decoder.layers`
- [ ] ç§»é™¤äº†æ‰€æœ‰ `position_ids` å‚æ•°
- [ ] Device map æ£€æŸ¥ä½¿ç”¨ `model.decoder.layers.{i}`
- [ ] `run_progressive_three_tier_universal.sh` å¯æ‰§è¡Œæƒé™å·²è®¾ç½®
- [ ] æ¨¡å‹è·¯å¾„å’Œæ•°æ®é›†é…ç½®æ­£ç¡®

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: æŠ¥é”™ `AttributeError: 'OPTForCausalLM' object has no attribute 'layers'`
**åŸå› **ï¼šä½¿ç”¨äº† Llama çš„å±‚è®¿é—®è·¯å¾„  
**è§£å†³**ï¼šæ”¹ä¸º `model.model.decoder.layers`

### Q2: æŠ¥é”™ `TypeError: forward() got an unexpected keyword argument 'position_ids'`
**åŸå› **ï¼šOPT ä¸ä½¿ç”¨ position_ids  
**è§£å†³**ï¼šç§»é™¤ forward è°ƒç”¨ä¸­çš„ `position_ids=position_ids`

### Q3: C4 æ•°æ®é›†åŠ è½½å¾ˆæ…¢
**åŸå› **ï¼šC4 æ•°æ®é›†è¾ƒå¤§ï¼ˆ~300GBï¼‰  
**è§£å†³**ï¼š
- é¦–æ¬¡åŠ è½½ä¼šä¸‹è½½å¹¶ç¼“å­˜
- å¯ä»¥ä½¿ç”¨ `--max_train_samples` é™åˆ¶æ ·æœ¬æ•°é‡
- æˆ–è€…å…ˆç”¨ WikiText2 æµ‹è¯•æµç¨‹

---

## ğŸ“ æ€»ç»“

### éœ€è¦åšçš„ä¿®æ”¹

1. **åˆ›å»ºæ–‡ä»¶**ï¼ˆå·²å®Œæˆï¼‰ï¼š
   - âœ… `run_progressive_three_tier_universal.sh`
   - âœ… `main_progressive_three_tier_opt.py`

2. **ä¿®æ”¹æ–‡ä»¶**ï¼ˆå¾…å®Œæˆï¼‰ï¼š
   - âš ï¸ `lib/prune_opt.py`ï¼šæ·»åŠ  progressive three-tier å‡½æ•°

3. **é…ç½®ä¿®æ”¹**ï¼š
   - ä¿®æ”¹ `run_progressive_three_tier_universal.sh` ä¸­çš„ï¼š
     - `MODEL_TYPE`
     - `BASE_MODEL`
     - `DATASET`

### è¿è¡Œä¸¤æ¬¡çš„é…ç½®

#### ç¬¬ä¸€æ¬¡ï¼šOPT + WikiText2
```bash
MODEL_TYPE="opt"
BASE_MODEL="/mnt/sdb/llm_models/opt-1.3b"
DATASET="wikitext2"
OUTPUT_BASE="out/progressive_three_tier_opt_wikitext2"
```

#### ç¬¬äºŒæ¬¡ï¼šOPT + C4
```bash
MODEL_TYPE="opt"
BASE_MODEL="/mnt/sdb/llm_models/opt-1.3b"
DATASET="c4"
OUTPUT_BASE="out/progressive_three_tier_opt_c4"
```

---

## ğŸ”— ç›¸å…³æ–‡ä»¶

- `lib/prune.py`ï¼šLlama ç‰ˆæœ¬çš„å‚è€ƒå®ç°
- `lib/prune_opt.py`ï¼šOPT ç‰ˆæœ¬ï¼ˆéœ€è¦æ·»åŠ å‡½æ•°ï¼‰
- `main_progressive_three_tier.py`ï¼šLlama ç‰ˆæœ¬çš„ä¸»è„šæœ¬
- `main_progressive_three_tier_opt.py`ï¼šOPT ç‰ˆæœ¬çš„ä¸»è„šæœ¬
- `progressive_config.csv`ï¼šè¿­ä»£é…ç½®ï¼ˆé€šç”¨ï¼‰

