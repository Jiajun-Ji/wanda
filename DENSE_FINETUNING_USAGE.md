# å…¨é‡å¾®è°ƒä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨ `run_dense_finetune_block_20sparsity.sh` è„šæœ¬å¯¹å‰ªæåçš„æ¨¡å‹è¿›è¡Œå…¨é‡å¾®è°ƒã€‚

è„šæœ¬æ”¯æŒ**å•å¡**å’Œ**å¤šå¡**è®­ç»ƒ,å¯ä»¥æ ¹æ®ä½ çš„GPUèµ„æºçµæ´»é€‰æ‹©ã€‚

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å•å¡è®­ç»ƒ (é»˜è®¤)

```bash
cd /home/jjji/Research/Hybird-Kernel/wanda
bash run_dense_finetune_block_20sparsity.sh
```

**é»˜è®¤é…ç½®**:
- GPU: å•å¡ (GPU 2)
- è®­ç»ƒæ—¶é—´: ~24-36å°æ—¶
- æ˜¾å­˜éœ€æ±‚: ~55-65GB

---

### åŒå¡è®­ç»ƒ (æ¨è)

```bash
bash run_dense_finetune_block_20sparsity.sh --num_gpus 2 --gpu_ids "2,3"
```

**é…ç½®**:
- GPU: åŒå¡ (GPU 2,3)
- è®­ç»ƒæ—¶é—´: ~14-20å°æ—¶ (1.7å€åŠ é€Ÿ)
- æ˜¾å­˜éœ€æ±‚: ~55-65GB (æ¯å¡)

---

### å››å¡è®­ç»ƒ

```bash
bash run_dense_finetune_block_20sparsity.sh --num_gpus 4 --gpu_ids "0,1,2,3"
```

**é…ç½®**:
- GPU: å››å¡ (GPU 0-3)
- è®­ç»ƒæ—¶é—´: ~8-12å°æ—¶ (2.5-3å€åŠ é€Ÿ)
- æ˜¾å­˜éœ€æ±‚: ~55-65GB (æ¯å¡)

---

## ğŸ“– å‚æ•°è¯´æ˜

### å‘½ä»¤è¡Œå‚æ•°

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | ç¤ºä¾‹ |
|------|------|--------|------|
| `--num_gpus` | GPUæ•°é‡ | 1 | `--num_gpus 2` |
| `--gpu_ids` | GPUç¼–å· (é€—å·åˆ†éš”) | "2" | `--gpu_ids "2,3"` |
| `-h, --help` | æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯ | - | `--help` |

### è®­ç»ƒå‚æ•° (è„šæœ¬å†…é…ç½®)

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| `NUM_EPOCHS` | 3 | è®­ç»ƒè½®æ•° |
| `LEARNING_RATE` | 5e-5 | å­¦ä¹ ç‡ |
| `BATCH_SIZE` | 1 | æ¯å¡batch size |
| `GRADIENT_ACCUMULATION_STEPS` | è‡ªåŠ¨è°ƒæ•´ | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° |
| `BLOCK_SIZE` | 1024 | åºåˆ—é•¿åº¦ |
| `MAX_TRAIN_SAMPLES` | 30000 | æœ€å¤§è®­ç»ƒæ ·æœ¬æ•° |

---

## ğŸ”§ æ¢¯åº¦ç´¯ç§¯è‡ªåŠ¨è°ƒæ•´

è„šæœ¬ä¼šæ ¹æ®GPUæ•°é‡è‡ªåŠ¨è°ƒæ•´æ¢¯åº¦ç´¯ç§¯æ­¥æ•°,ä»¥ä¿æŒ**ç›¸åŒçš„æœ‰æ•ˆbatch size**:

| GPUæ•°é‡ | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° | æœ‰æ•ˆBatch Size |
|---------|-------------|---------------|
| 1 | 8 | 1 Ã— 1 Ã— 8 = 8 |
| 2 | 4 | 1 Ã— 2 Ã— 4 = 8 |
| 4 | 2 | 1 Ã— 4 Ã— 2 = 8 |

**å…¬å¼**: `æœ‰æ•ˆBatch Size = per_device_batch_size Ã— num_gpus Ã— gradient_accumulation_steps`

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### è®­ç»ƒæ—¶é—´ä¼°ç®—

| GPUé…ç½® | è®­ç»ƒæ—¶é—´ | åŠ é€Ÿæ¯” | æ•ˆç‡ |
|---------|---------|--------|------|
| 1Ã—A100 80GB | 24-36h | 1.0Ã— | 100% |
| 2Ã—A100 80GB | 14-20h | 1.7Ã— | 85% |
| 4Ã—A100 80GB | 8-12h | 2.5-3Ã— | 65-75% |

### æ˜¾å­˜éœ€æ±‚

æ¯å¼ GPUéœ€è¦:
- æ¨¡å‹å‚æ•° (FP16): ~13GB
- ä¼˜åŒ–å™¨çŠ¶æ€ (AdamW): ~26GB
- æ¢¯åº¦: ~13GB
- æ¿€æ´»å€¼ (gradient checkpointing): ~5-10GB
- **æ€»è®¡**: ~55-65GB

**æ¨èé…ç½®**:
- âœ… A100 80GB (æœ€ä½³)
- âœ… A100 40GB (å¯ç”¨,éœ€gradient checkpointing)
- âš ï¸ V100 32GB (ä¸å¤Ÿ)

---

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1: å•å¡è®­ç»ƒ (GPU 3)

```bash
bash run_dense_finetune_block_20sparsity.sh --num_gpus 1 --gpu_ids "3"
```

### ç¤ºä¾‹2: åŒå¡è®­ç»ƒ (GPU 0,1)

```bash
bash run_dense_finetune_block_20sparsity.sh --num_gpus 2 --gpu_ids "0,1"
```

### ç¤ºä¾‹3: æŸ¥çœ‹å¸®åŠ©

```bash
bash run_dense_finetune_block_20sparsity.sh --help
```

è¾“å‡º:
```
Usage: run_dense_finetune_block_20sparsity.sh [OPTIONS]

Options:
  --num_gpus N        Number of GPUs to use (default: 1)
  --gpu_ids "X,Y"     Comma-separated GPU IDs (default: "2")
  -h, --help          Show this help message

Examples:
  Single GPU (default):     ./run_dense_finetune_block_20sparsity.sh
  Single GPU (GPU 3):       ./run_dense_finetune_block_20sparsity.sh --num_gpus 1 --gpu_ids "3"
  Dual GPU (GPU 2,3):       ./run_dense_finetune_block_20sparsity.sh --num_gpus 2 --gpu_ids "2,3"
  Quad GPU (GPU 0-3):       ./run_dense_finetune_block_20sparsity.sh --num_gpus 4 --gpu_ids "0,1,2,3"

Performance:
  1 GPU: ~24-36 hours
  2 GPUs: ~14-20 hours (1.7x speedup)
  4 GPUs: ~8-12 hours (2.5-3x speedup)
```

---

## ğŸ” è®­ç»ƒè¿‡ç¨‹ç›‘æ§

### è®­ç»ƒå¼€å§‹æ—¶çš„è¾“å‡º

```
==========================================
Dense Fine-tuning Configuration
==========================================
Method: Full fine-tuning with SparseTrainer
Pruned model: out/llama2_7b/block_16x16_20sparsity/wanda/pruned_model
Output directory: out/llama2_7b/block_16x16_20sparsity/wanda/dense_finetuned_model
Dataset: wikitext (wikitext-2-raw-v1)

GPU Configuration:
  Number of GPUs: 2
  GPU IDs: 2,3
  Training mode: Multi-GPU (torchrun)

Training Parameters:
  Number of epochs: 3
  Learning rate: 5e-5
  Per-device batch size: 1
  Gradient accumulation: 4
  Effective batch size: 8
  Block size: 1024
  Max train samples: 30000
==========================================

ğŸš€ Starting dense fine-tuning with SparseTrainer...
Training mode: 2 GPU(s) on device(s): 2,3
Effective batch size: 8
==========================================
Using multi-GPU training with torchrun (2 GPUs)...
```

### è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¾“å‡º

```
*** Starting sparse full fine-tuning ***
Initial model sparsity: 0.2000
Trainable params: 6,738,415,616 || All params: 6,738,415,616 || Trainable%: 100.00

Step 10: loss=2.5432
Step 20: loss=2.3456
Step 30: loss=2.1234
...
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. GPUå†…å­˜æ£€æŸ¥

è„šæœ¬ä¼šè‡ªåŠ¨æ£€æŸ¥GPUå†…å­˜:

```bash
Available GPU memory: 81920 MB (~80 GB)
```

å¦‚æœå†…å­˜ä¸è¶³ (<40GB),ä¼šæç¤º:

```
âš ï¸  Warning: GPU memory may be insufficient!
Consider using LoRA fine-tuning instead.
Continue anyway? (y/n)
```

### 2. å¤šå¡è®­ç»ƒè¦æ±‚

- æ‰€æœ‰GPUå¿…é¡»æœ‰è¶³å¤Ÿçš„æ˜¾å­˜ (â‰¥40GB)
- GPUä¹‹é—´éœ€è¦é«˜é€Ÿäº’è” (NVLinkæ¨è)
- éœ€è¦å®‰è£… `torch.distributed`

### 3. å­¦ä¹ ç‡è°ƒæ•´

å¦‚æœä¿®æ”¹äº†æœ‰æ•ˆbatch size,å¯èƒ½éœ€è¦è°ƒæ•´å­¦ä¹ ç‡:

```bash
# çº¿æ€§ç¼©æ”¾è§„åˆ™
# å¦‚æœæœ‰æ•ˆbatch sizeä»8å¢å¤§åˆ°16
# å­¦ä¹ ç‡ä»5e-5å¢å¤§åˆ°1e-4
```

---

## ğŸ“ è¾“å‡ºæ–‡ä»¶

è®­ç»ƒå®Œæˆå,æ¨¡å‹ä¿å­˜åœ¨:

```
out/llama2_7b/block_16x16_20sparsity/wanda/dense_finetuned_model/
â”œâ”€â”€ config.json
â”œâ”€â”€ generation_config.json
â”œâ”€â”€ pytorch_model.bin  (~13GB)
â”œâ”€â”€ special_tokens_map.json
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer_config.json
â””â”€â”€ trainer_state.json
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥

è®­ç»ƒå®Œæˆå:

1. **è¯„ä¼°æ¨¡å‹**:
   ```bash
   python main_block.py --model out/llama2_7b/block_16x16_20sparsity/wanda/dense_finetuned_model --eval_zero_shot
   ```

2. **å¯¹æ¯”æ€§èƒ½**:
   - å‰ªæå: å›°æƒ‘åº¦ 135.44
   - LoRAå¾®è°ƒ: å›°æƒ‘åº¦ 56.82
   - å…¨é‡å¾®è°ƒ: å›°æƒ‘åº¦ 40-50 (é¢„æœŸ)

3. **éªŒè¯ç¨€ç–æ€§**:
   ```python
   from dense_ft.sparse_trainer import check_sparsity
   model = AutoModelForCausalLM.from_pretrained("out/.../dense_finetuned_model")
   sparsity = check_sparsity(model)
   print(f"Sparsity: {sparsity:.4f}")  # åº”è¯¥ä»ç„¶æ˜¯ ~0.20
   ```

---

## ğŸ†š LoRA vs å…¨é‡å¾®è°ƒå¯¹æ¯”

| ç‰¹æ€§ | LoRAå¾®è°ƒ | å…¨é‡å¾®è°ƒ |
|------|---------|---------|
| å¯è®­ç»ƒå‚æ•° | <1% (~4M) | 100% (~7B) |
| GPUå†…å­˜ | ~20GB | ~55-65GB |
| è®­ç»ƒæ—¶é—´ (å•å¡) | ~12h | ~24-36h |
| è®­ç»ƒæ—¶é—´ (åŒå¡) | ~7h | ~14-20h |
| æ•ˆæœ | å¥½ (56.82) | æ›´å¥½ (40-50é¢„æœŸ) |
| ä¿å­˜å¤§å° | ~16MB | ~13GB |
| æ¨èåœºæ™¯ | å¿«é€Ÿå®éªŒ | è¿½æ±‚æœ€ä½³æ€§èƒ½ |

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•æŸ¥çœ‹å¯ç”¨çš„GPU?

```bash
nvidia-smi
```

### Q2: å¦‚ä½•åœæ­¢è®­ç»ƒ?

æŒ‰ `Ctrl+C`,æ¨¡å‹ä¼šä¿å­˜æœ€åä¸€ä¸ªcheckpointã€‚

### Q3: å¦‚ä½•ä»checkpointæ¢å¤è®­ç»ƒ?

è„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä»æœ€åçš„checkpointæ¢å¤ã€‚

### Q4: å¤šå¡è®­ç»ƒæ¯”å•å¡æ…¢?

æ£€æŸ¥:
- GPUä¹‹é—´æ˜¯å¦æœ‰NVLinkè¿æ¥
- æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹å ç”¨GPU
- ç½‘ç»œé€šä¿¡æ˜¯å¦æ­£å¸¸

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [LoRAå¾®è°ƒæŒ‡å—](FINETUNING_GUIDE_CN.md)
- [å‰ªææŒ‡å—](md/prune/QUICKSTART.md)
- [é¡¹ç›®æ¦‚è§ˆ](md/prune/PROJECT_OVERVIEW.md)

