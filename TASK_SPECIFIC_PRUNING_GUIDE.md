# ä»»åŠ¡ç‰¹å®šå‰ªæä¸å¾®è°ƒæŒ‡å—

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•ä½¿ç”¨ç‰¹å®šä»»åŠ¡æ•°æ®é›†ï¼ˆå¦‚ SQuADã€GSM8Kï¼‰è¿›è¡Œå‰ªæå’Œå¾®è°ƒã€‚

---

## ğŸ“‹ æ ¸å¿ƒé—®é¢˜

### Q1: å‰ªææ—¶èƒ½ç”¨ä»»åŠ¡æ•°æ®é›†ä½œä¸ºæ ¡å‡†æ•°æ®å—ï¼Ÿ

**A: å¯ä»¥ï¼Œä½†éœ€è¦æƒè¡¡ã€‚**

#### âœ… ä¼˜ç‚¹
- å‰ªæåçš„æ¨¡å‹åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šæ€§èƒ½æ›´å¥½
- æ¿€æ´»å€¼æ›´è´´è¿‘å®é™…ä½¿ç”¨åœºæ™¯
- å¯èƒ½ä¿ç•™æ›´å¤šä»»åŠ¡ç›¸å…³çš„æƒé‡

#### âš ï¸ ç¼ºç‚¹
- å¤±å»æ¨¡å‹çš„é€šç”¨èƒ½åŠ›
- å¯èƒ½åœ¨å…¶ä»–ä»»åŠ¡ä¸Šæ€§èƒ½ä¸‹é™
- éœ€è¦ä¿®æ”¹ä»£ç æ·»åŠ æ•°æ®é›†æ”¯æŒ

#### ğŸ“Š å®éªŒå¯¹æ¯”

| æ ¡å‡†æ•°æ® | ç›®æ ‡ä»»åŠ¡æ€§èƒ½ | é€šç”¨æ€§èƒ½ | é€‚ç”¨åœºæ™¯ |
|---------|------------|---------|---------|
| **WikiText2** | ä¸­ç­‰ | é«˜ | é€šç”¨æ¨¡å‹ |
| **ä»»åŠ¡æ•°æ®é›†** | é«˜ | ä½ | ç‰¹å®šä»»åŠ¡ |
| **æ··åˆæ•°æ®** | è¾ƒé«˜ | ä¸­ç­‰ | æŠ˜ä¸­æ–¹æ¡ˆ |

---

### Q2: å¾®è°ƒæ—¶èƒ½ç”¨ä»»åŠ¡æ•°æ®é›†å—ï¼Ÿ

**A: å¯ä»¥ï¼Œè€Œä¸”æ¨èï¼**

#### âœ… ä¼˜ç‚¹
- é’ˆå¯¹æ€§ä¼˜åŒ–ï¼Œæ€§èƒ½æå‡æ˜æ˜¾
- å¯ä»¥ä½¿ç”¨ä»»åŠ¡ç‰¹å®šçš„è®­ç»ƒç­–ç•¥
- æ›´ç¬¦åˆå®é™…åº”ç”¨éœ€æ±‚

#### âš ï¸ æ³¨æ„äº‹é¡¹
- éœ€è¦ç‰¹æ®Šçš„ prompt æ ¼å¼
- éœ€è¦ä¿®æ”¹è®­ç»ƒè„šæœ¬
- å¯èƒ½éœ€è¦æ›´å¤šçš„è®­ç»ƒæ•°æ®

---

## ğŸ¯ æ¨èç­–ç•¥

### ç­–ç•¥ 1: é€šç”¨å‰ªæ + ä»»åŠ¡å¾®è°ƒï¼ˆæ¨èï¼‰

```
WikiText2 å‰ªæ â†’ ä»»åŠ¡æ•°æ®å¾®è°ƒ â†’ ä»»åŠ¡è¯„ä¼°
```

**ä¼˜ç‚¹**ï¼š
- âœ… ä¿æŒæ¨¡å‹é€šç”¨æ€§
- âœ… é’ˆå¯¹ä»»åŠ¡ä¼˜åŒ–
- âœ… å¹³è¡¡æ€§èƒ½å’Œé€šç”¨æ€§

**é€‚ç”¨åœºæ™¯**ï¼š
- éœ€è¦åœ¨å¤šä¸ªä»»åŠ¡ä¸Šä½¿ç”¨
- å¸Œæœ›ä¿æŒä¸€å®šçš„é€šç”¨èƒ½åŠ›
- æœ‰è¶³å¤Ÿçš„ä»»åŠ¡æ•°æ®è¿›è¡Œå¾®è°ƒ

### ç­–ç•¥ 2: ä»»åŠ¡å‰ªæ + ä»»åŠ¡å¾®è°ƒ

```
ä»»åŠ¡æ•°æ®å‰ªæ â†’ ä»»åŠ¡æ•°æ®å¾®è°ƒ â†’ ä»»åŠ¡è¯„ä¼°
```

**ä¼˜ç‚¹**ï¼š
- âœ… æœ€å¤§åŒ–ä»»åŠ¡æ€§èƒ½
- âœ… ç«¯åˆ°ç«¯ä¼˜åŒ–

**ç¼ºç‚¹**ï¼š
- âš ï¸ å¤±å»é€šç”¨æ€§
- âš ï¸ å¯èƒ½è¿‡æ‹Ÿåˆ

**é€‚ç”¨åœºæ™¯**ï¼š
- åªå…³å¿ƒç‰¹å®šä»»åŠ¡
- æœ‰å¤§é‡ä»»åŠ¡æ•°æ®
- è¿½æ±‚æè‡´æ€§èƒ½

### ç­–ç•¥ 3: æ··åˆå‰ªæ + ä»»åŠ¡å¾®è°ƒ

```
(WikiText2 + ä»»åŠ¡æ•°æ®) å‰ªæ â†’ ä»»åŠ¡æ•°æ®å¾®è°ƒ â†’ ä»»åŠ¡è¯„ä¼°
```

**ä¼˜ç‚¹**ï¼š
- âœ… å¹³è¡¡é€šç”¨æ€§å’Œä»»åŠ¡æ€§èƒ½
- âœ… æ›´ç¨³å¥

**é€‚ç”¨åœºæ™¯**ï¼š
- éœ€è¦åœ¨å¤šä¸ªç›¸å…³ä»»åŠ¡ä¸Šä½¿ç”¨
- å¸Œæœ›ä¿æŒä¸€å®šçš„é€šç”¨èƒ½åŠ›

---

## ğŸ”§ å®ç°æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: æ·»åŠ ä»»åŠ¡æ•°æ®é›†æ”¯æŒåˆ°å‰ªæ

#### æ­¥éª¤ 1: ä¿®æ”¹ `lib/data.py`

æ·»åŠ  SQuAD å’Œ GSM8K æ•°æ®åŠ è½½å‡½æ•°ï¼š

```python
# æ·»åŠ åˆ° lib/data.py

def get_squad(nsamples, seed, seqlen, tokenizer):
    """
    åŠ è½½ SQuAD æ•°æ®é›†ç”¨äºå‰ªææ ¡å‡†
    ä½¿ç”¨ context + question ä½œä¸ºè¾“å…¥
    """
    from datasets import load_dataset
    
    # åŠ è½½ SQuAD v2
    dataset = load_dataset('squad_v2', split='train')
    
    # ç”Ÿæˆæ ·æœ¬
    random.seed(seed)
    trainloader = []
    
    for _ in range(nsamples):
        # éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬
        i = random.randint(0, len(dataset) - 1)
        sample = dataset[i]
        
        # æ„é€ è¾“å…¥ï¼šcontext + question
        text = f"Context: {sample['context']}\nQuestion: {sample['question']}\nAnswer:"
        
        # Tokenize
        enc = tokenizer(text, return_tensors='pt', max_length=seqlen, truncation=True)
        inp = enc.input_ids
        
        # åˆ›å»º targetï¼ˆå‰ªææ—¶ä¸ä½¿ç”¨ï¼Œä½†ä¿æŒæ ¼å¼ä¸€è‡´ï¼‰
        tar = inp.clone()
        tar[:, :-1] = -100
        
        trainloader.append((inp, tar))
    
    # è¿”å›è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ï¼ˆä½¿ç”¨ WikiText2 ä½œä¸ºæµ‹è¯•ï¼‰
    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test', verification_mode='no_checks')
    testenc = tokenizer("\n\n".join(testdata['text']), return_tensors='pt')
    
    return trainloader, testenc


def get_gsm8k(nsamples, seed, seqlen, tokenizer):
    """
    åŠ è½½ GSM8K æ•°æ®é›†ç”¨äºå‰ªææ ¡å‡†
    ä½¿ç”¨ question (+ answer) ä½œä¸ºè¾“å…¥
    """
    from datasets import load_dataset
    
    # åŠ è½½ GSM8K
    dataset = load_dataset('gsm8k', 'main', split='train')
    
    # ç”Ÿæˆæ ·æœ¬
    random.seed(seed)
    trainloader = []
    
    for _ in range(nsamples):
        # éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬
        i = random.randint(0, len(dataset) - 1)
        sample = dataset[i]
        
        # æ„é€ è¾“å…¥ï¼šquestion + answerï¼ˆç”¨äºå‰ªææ—¶è®¡ç®—æ¿€æ´»å€¼ï¼‰
        text = f"Question: {sample['question']}\nAnswer: {sample['answer']}"
        
        # Tokenize
        enc = tokenizer(text, return_tensors='pt', max_length=seqlen, truncation=True)
        inp = enc.input_ids
        
        # åˆ›å»º target
        tar = inp.clone()
        tar[:, :-1] = -100
        
        trainloader.append((inp, tar))
    
    # è¿”å›è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®
    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test', verification_mode='no_checks')
    testenc = tokenizer("\n\n".join(testdata['text']), return_tensors='pt')
    
    return trainloader, testenc


# ä¿®æ”¹ get_loaders å‡½æ•°
def get_loaders(name, nsamples=128, seed=0, seqlen=2048, tokenizer=None):
    if 'wikitext2' in name:
        return get_wikitext2(nsamples, seed, seqlen, tokenizer)
    if "c4" in name:
        return get_c4(nsamples, seed, seqlen, tokenizer)
    if "squad" in name:
        return get_squad(nsamples, seed, seqlen, tokenizer)
    if "gsm8k" in name:
        return get_gsm8k(nsamples, seed, seqlen, tokenizer)
    raise ValueError(f"Unknown dataset: {name}")
```

#### æ­¥éª¤ 2: ä½¿ç”¨ä»»åŠ¡æ•°æ®é›†å‰ªæ

```bash
# ä½¿ç”¨ SQuAD æ•°æ®é›†å‰ªæ
python main_block_three_tier.py \
    --model /mnt/sdb/llm_models/Llama-2-7b-hf \
    --sparsity_ratios 0.35 0.45 0.2 \
    --nsamples 128 \
    --save out/llama2_7b/squad_pruned/ \
    --calibration_dataset squad  # æ–°å¢å‚æ•°

# ä½¿ç”¨ GSM8K æ•°æ®é›†å‰ªæ
python main_block_three_tier.py \
    --model /mnt/sdb/llm_models/Llama-2-7b-hf \
    --sparsity_ratios 0.35 0.45 0.2 \
    --nsamples 128 \
    --save out/llama2_7b/gsm8k_pruned/ \
    --calibration_dataset gsm8k  # æ–°å¢å‚æ•°
```

**æ³¨æ„**ï¼šéœ€è¦åœ¨ `main_block_three_tier.py` ä¸­æ·»åŠ  `--calibration_dataset` å‚æ•°ã€‚

---

### æ–¹æ¡ˆ 2: ä½¿ç”¨ä»»åŠ¡æ•°æ®é›†å¾®è°ƒ

#### æ­¥éª¤ 1: åˆ›å»ºä»»åŠ¡ç‰¹å®šçš„å¾®è°ƒè„šæœ¬

åˆ›å»º `dense_ft/finetune_squad.py`ï¼š

```python
#!/usr/bin/env python3
"""
ä½¿ç”¨ SQuAD æ•°æ®é›†å¾®è°ƒå‰ªæåçš„æ¨¡å‹
"""

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset

def preprocess_squad(examples, tokenizer, max_length=512):
    """
    é¢„å¤„ç† SQuAD æ•°æ®
    æ ¼å¼ï¼šContext: ... Question: ... Answer: ...
    """
    inputs = []
    for context, question, answers in zip(
        examples['context'],
        examples['question'],
        examples['answers']
    ):
        # æå–ç­”æ¡ˆæ–‡æœ¬
        answer_text = answers['text'][0] if answers['text'] else "No answer"
        
        # æ„é€ è¾“å…¥
        text = f"Context: {context}\nQuestion: {question}\nAnswer: {answer_text}"
        inputs.append(text)
    
    # Tokenize
    model_inputs = tokenizer(
        inputs,
        max_length=max_length,
        truncation=True,
        padding='max_length',
        return_tensors='pt'
    )
    
    # è®¾ç½® labelsï¼ˆç”¨äºè®¡ç®—æŸå¤±ï¼‰
    model_inputs['labels'] = model_inputs['input_ids'].clone()
    
    return model_inputs

# ä¸»å‡½æ•°
def main():
    # åŠ è½½æ¨¡å‹å’Œ tokenizer
    model_path = "out/llama2_7b/pruned_model"
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map='auto'
    )
    tokenizer = AutoTokenizer.from_pretrained("/mnt/sdb/llm_models/Llama-2-7b-hf")
    tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½ SQuAD æ•°æ®é›†
    dataset = load_dataset('squad_v2')
    
    # é¢„å¤„ç†æ•°æ®
    train_dataset = dataset['train'].map(
        lambda x: preprocess_squad(x, tokenizer),
        batched=True,
        remove_columns=dataset['train'].column_names
    )
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir="out/llama2_7b/squad_finetuned",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=2e-5,
        bf16=True,
        logging_steps=10,
        save_steps=100,
        eval_strategy="steps",
        eval_steps=100,
        save_total_limit=2,
        load_best_model_at_end=True,
    )
    
    # åˆ›å»º Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model()

if __name__ == "__main__":
    main()
```

#### æ­¥éª¤ 2: è¿è¡Œä»»åŠ¡å¾®è°ƒ

```bash
cd /home/jjji/Research/Hybird-Kernel/wanda/dense_ft

# å¾®è°ƒ SQuAD
python finetune_squad.py

# å¾®è°ƒ GSM8K
python finetune_gsm8k.py
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”å®éªŒ

### å®éªŒè®¾è®¡

| å®éªŒ | å‰ªææ•°æ® | å¾®è°ƒæ•°æ® | è¯„ä¼°ä»»åŠ¡ |
|------|---------|---------|---------|
| **Baseline** | WikiText2 | WikiText2 | SQuAD, GSM8K, BoolQ |
| **Exp 1** | WikiText2 | SQuAD | SQuAD, GSM8K, BoolQ |
| **Exp 2** | SQuAD | SQuAD | SQuAD, GSM8K, BoolQ |
| **Exp 3** | WikiText2 | GSM8K | SQuAD, GSM8K, BoolQ |
| **Exp 4** | GSM8K | GSM8K | SQuAD, GSM8K, BoolQ |

### é¢„æœŸç»“æœ

- **Exp 1**: SQuAD â†‘, GSM8K â‰ˆ, BoolQ â†“
- **Exp 2**: SQuAD â†‘â†‘, GSM8K â†“, BoolQ â†“â†“
- **Exp 3**: GSM8K â†‘, SQuAD â‰ˆ, BoolQ â†“
- **Exp 4**: GSM8K â†‘â†‘, SQuAD â†“, BoolQ â†“â†“

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„ç­–ç•¥

- **é€šç”¨æ¨¡å‹**: ä½¿ç”¨ WikiText2 å‰ªæ + WikiText2 å¾®è°ƒ
- **å•ä»»åŠ¡ä¼˜åŒ–**: ä½¿ç”¨ä»»åŠ¡æ•°æ®å‰ªæ + ä»»åŠ¡æ•°æ®å¾®è°ƒ
- **å¤šä»»åŠ¡å¹³è¡¡**: ä½¿ç”¨ WikiText2 å‰ªæ + ä»»åŠ¡æ•°æ®å¾®è°ƒ

### 2. æ•°æ®é‡å»ºè®®

| é˜¶æ®µ | æ¨èæ ·æœ¬æ•° | è¯´æ˜ |
|------|-----------|------|
| **å‰ªææ ¡å‡†** | 128-256 | è¶³å¤Ÿè®¡ç®—æ¿€æ´»å€¼ |
| **å¾®è°ƒè®­ç»ƒ** | 1000+ | è¶Šå¤šè¶Šå¥½ |
| **è¯„ä¼°æµ‹è¯•** | å…¨é‡ | ä½¿ç”¨å®Œæ•´æµ‹è¯•é›† |

### 3. è¶…å‚æ•°è°ƒæ•´

```python
# å‰ªæ
--nsamples 128          # æ ¡å‡†æ ·æœ¬æ•°
--seqlen 2048           # åºåˆ—é•¿åº¦

# å¾®è°ƒ
--num_train_epochs 3    # è®­ç»ƒè½®æ•°
--learning_rate 2e-5    # å­¦ä¹ ç‡
--batch_size 4          # æ‰¹æ¬¡å¤§å°
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. æ•°æ®æ ¼å¼

ä¸åŒä»»åŠ¡çš„æ•°æ®æ ¼å¼ä¸åŒï¼Œéœ€è¦æ­£ç¡®å¤„ç†ï¼š

- **SQuAD**: `{"context": "...", "question": "...", "answers": {...}}`
- **GSM8K**: `{"question": "...", "answer": "..."}`
- **WikiText2**: çº¯æ–‡æœ¬

### 2. Prompt è®¾è®¡

ä»»åŠ¡ç‰¹å®šå¾®è°ƒéœ€è¦è®¾è®¡åˆé€‚çš„ promptï¼š

```python
# SQuAD
prompt = f"Context: {context}\nQuestion: {question}\nAnswer: {answer}"

# GSM8K
prompt = f"Question: {question}\nLet's solve this step by step:\n{answer}"
```

### 3. è¯„ä¼°æŒ‡æ ‡

ä¸åŒä»»åŠ¡ä½¿ç”¨ä¸åŒçš„è¯„ä¼°æŒ‡æ ‡ï¼š

- **SQuAD**: Exact Match, F1 Score
- **GSM8K**: Accuracy
- **WikiText2**: Perplexity

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [Wanda è®ºæ–‡](https://arxiv.org/abs/2306.11695)
- [SQuAD æ•°æ®é›†](https://rajpurkar.github.io/SQuAD-explorer/)
- [GSM8K æ•°æ®é›†](https://github.com/openai/grade-school-math)
- [Hugging Face Datasets](https://huggingface.co/docs/datasets)

---

**æœ€åæ›´æ–°**: 2025-01-XX  
**ç»´æŠ¤è€…**: Jiajun Ji  
**é¡¹ç›®**: Wanda Hybrid Pruning

