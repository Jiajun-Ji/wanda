# Wandaå‰ªæåæ¢å¤ç²¾åº¦æ–¹æ³•è¯¦è§£

## ğŸ“‹ æ¦‚è¿°

Wandaé¡¹ç›®æä¾›äº†**ä¸¤ç§**å‰ªæåæ¢å¤ç²¾åº¦çš„æ–¹æ³•:

1. **LoRAå¾®è°ƒ** (Parameter-Efficient Fine-Tuning) - **æ¨èæ–¹æ³•**
2. **Denseå¾®è°ƒ** (Full Fine-Tuning with Sparse Constraints)

## ğŸ” æ ¸å¿ƒåŒºåˆ«

### æ–¹æ³•å¯¹æ¯”

| ç‰¹æ€§ | LoRAå¾®è°ƒ | Denseå¾®è°ƒ |
|------|---------|----------|
| **è®­ç»ƒå‚æ•°é‡** | æå°‘(~0.1%) | å…¨éƒ¨éé›¶å‚æ•°(~50%) |
| **æ˜¾å­˜éœ€æ±‚** | ä½ | é«˜ |
| **è®­ç»ƒé€Ÿåº¦** | å¿« | æ…¢ |
| **ä¿æŒç¨€ç–æ€§** | âœ… è‡ªåŠ¨ä¿æŒ | âœ… é€šè¿‡æ¢¯åº¦maskä¿æŒ |
| **é€‚ç”¨åœºæ™¯** | èµ„æºå—é™,å¿«é€Ÿå¾®è°ƒ | å……è¶³èµ„æº,è¿½æ±‚æè‡´æ€§èƒ½ |
| **Wandaæ¨è** | âœ… ä¸»è¦æ–¹æ³• | å¤‡é€‰æ–¹æ³• |

## ğŸ¯ æ–¹æ³•1: LoRAå¾®è°ƒ (æ¨è)

### åŸç†è¯´æ˜

**LoRA (Low-Rank Adaptation)** æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•:

1. **å†»ç»“åŸå§‹æƒé‡**: å‰ªæåçš„ç¨€ç–æƒé‡ä¿æŒä¸å˜
2. **æ·»åŠ ä½ç§©çŸ©é˜µ**: åœ¨attentionå±‚æ·»åŠ å¯è®­ç»ƒçš„ä½ç§©åˆ†è§£çŸ©é˜µ
3. **åªè®­ç»ƒLoRAå‚æ•°**: ä»…è®­ç»ƒæ–°å¢çš„å°‘é‡å‚æ•°(é€šå¸¸<1%æ€»å‚æ•°)
4. **æ¨ç†æ—¶åˆå¹¶**: å¯é€‰æ‹©å°†LoRAæƒé‡åˆå¹¶å›åŸå§‹æ¨¡å‹

### ä»£ç å®ç°

<augment_code_snippet path="wanda/lora_ft/finetune_lm.py" mode="EXCERPT">
````python
# å…³é”®ä»£ç ç‰‡æ®µ
model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, ...)

# å‡†å¤‡LoRAè®­ç»ƒ
model = prepare_model_for_int8_training(model)

# LoRAé…ç½®
config = LoraConfig(
    r=8,                              # LoRAç§©(rank)
    lora_alpha=16,                    # LoRAç¼©æ”¾å› å­
    target_modules=["q_proj","v_proj"], # ç›®æ ‡æ¨¡å—(attentionçš„Qå’ŒV)
    lora_dropout=0.05,                # Dropoutç‡
    bias="none",                      # ä¸è®­ç»ƒbias
    task_type="CAUSAL_LM",           # ä»»åŠ¡ç±»å‹
)

# åº”ç”¨LoRA
model = get_peft_model(model, config)
````
</augment_code_snippet>

### ä½¿ç”¨æ­¥éª¤

#### 1. å‡†å¤‡ç¯å¢ƒ

```bash
pip install peft  # å®‰è£…PEFTåº“(åŒ…å«LoRA)
```

#### 2. è¿è¡ŒLoRAå¾®è°ƒ

```bash
cd /home/jjji/Research/Hybird-Kernel/wanda/lora_ft

CUDA_VISIBLE_DEVICES=0 python finetune_lm.py \
    --model_name_or_path /home/jjji/Research/Hybird-Kernel/wanda/out/llama2_7b/unstructured/wanda/pruned_model \
    --config_name "meta-llama/Llama-2-7b-hf" \
    --dataset_name c4 \
    --num_train_epochs 1 \
    --block_size 1024 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --max_train_samples 30000 \
    --max_eval_samples 128 \
    --learning_rate 1e-4 \
    --overwrite_output_dir \
    --output_dir /home/jjji/Research/Hybird-Kernel/wanda/out/llama2_7b/unstructured/wanda/lora_weights
```

#### 3. è¯„ä¼°LoRAå¾®è°ƒåçš„æ¨¡å‹

```bash
cd /home/jjji/Research/Hybird-Kernel/wanda/lora_ft

CUDA_VISIBLE_DEVICES=0 python evaluate_ppl.py \
    --model /home/jjji/Research/Hybird-Kernel/wanda/out/llama2_7b/unstructured/wanda/pruned_model \
    --lora_weights /home/jjji/Research/Hybird-Kernel/wanda/out/llama2_7b/unstructured/wanda/lora_weights
```

### å‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `--model_name_or_path` | å‰ªææ¨¡å‹è·¯å¾„ | ä½ çš„pruned_modelè·¯å¾„ |
| `--config_name` | æ¨¡å‹é…ç½®åç§° | `meta-llama/Llama-2-7b-hf` |
| `--dataset_name` | è®­ç»ƒæ•°æ®é›† | `c4` æˆ– `wikitext` |
| `--num_train_epochs` | è®­ç»ƒè½®æ•° | `1` |
| `--block_size` | ä¸Šä¸‹æ–‡é•¿åº¦ | `1024` (80GB GPUå¯ç”¨2048) |
| `--max_train_samples` | è®­ç»ƒæ ·æœ¬æ•° | `30000` (~12å°æ—¶) |
| `--learning_rate` | å­¦ä¹ ç‡ | `1e-4` |
| `lora_r` | LoRAç§© | `8` |
| `lora_alpha` | LoRAç¼©æ”¾ | `16` |

### LoRAä¼˜åŠ¿

âœ… **æ˜¾å­˜å‹å¥½**: åªéœ€è®­ç»ƒ<1%çš„å‚æ•°  
âœ… **è®­ç»ƒå¿«é€Ÿ**: æ¯”å…¨é‡å¾®è°ƒå¿«5-10å€  
âœ… **ä¿æŒç¨€ç–**: åŸå§‹ç¨€ç–æƒé‡å®Œå…¨ä¸å˜  
âœ… **æ˜“äºéƒ¨ç½²**: LoRAæƒé‡å¯ä»¥ç‹¬ç«‹ä¿å­˜å’ŒåŠ è½½  
âœ… **å¤šä»»åŠ¡é€‚é…**: å¯ä»¥ä¸ºä¸åŒä»»åŠ¡è®­ç»ƒä¸åŒçš„LoRAæƒé‡  

## ğŸ¯ æ–¹æ³•2: Denseå¾®è°ƒ (ç¨€ç–çº¦æŸ)

### åŸç†è¯´æ˜

**Denseå¾®è°ƒ**æ˜¯åœ¨ä¿æŒç¨€ç–æ€§çº¦æŸä¸‹çš„å…¨é‡å¾®è°ƒ:

1. **è®­ç»ƒæ‰€æœ‰éé›¶æƒé‡**: æ›´æ–°æ‰€æœ‰æœªè¢«å‰ªæçš„å‚æ•°
2. **æ¢¯åº¦mask**: åœ¨åå‘ä¼ æ’­å,å°†è¢«å‰ªæä½ç½®çš„æ¢¯åº¦ç½®é›¶
3. **ä¿æŒç¨€ç–æ¨¡å¼**: ç¡®ä¿å‰ªæçš„æƒé‡å§‹ç»ˆä¸º0

### ä»£ç å®ç°

<augment_code_snippet path="wanda/dense_ft/sparse_trainer.py" mode="EXCERPT">
````python
def mask_grad(model):
    """åœ¨åå‘ä¼ æ’­åmaskæ‰è¢«å‰ªææƒé‡çš„æ¢¯åº¦"""
    layers = model.model.layers
    for i in range(len(layers)):
        layer = layers[i]
        subset = find_layers(layer)
        
        for name in subset:
            W = subset[name].weight.data
            mask = (W==0)  # æ‰¾åˆ°è¢«å‰ªæçš„ä½ç½®
            subset[name].weight.grad[mask] = 0  # æ¢¯åº¦ç½®é›¶

class SparseTrainer(Trainer):
    def training_step(self, model, inputs):
        # ... æ­£å¸¸çš„å‰å‘å’Œåå‘ä¼ æ’­ ...
        self.accelerator.backward(loss)
        
        # å…³é”®æ­¥éª¤: maskæ‰è¢«å‰ªææƒé‡çš„æ¢¯åº¦
        mask_grad(model)
        
        return loss.detach()
````
</augment_code_snippet>

### ä½¿ç”¨æ­¥éª¤

Denseå¾®è°ƒéœ€è¦è‡ªå·±å®ç°è®­ç»ƒå¾ªç¯,ä½¿ç”¨`SparseTrainer`æ›¿ä»£æ ‡å‡†çš„`Trainer`:

```python
from dense_ft.sparse_trainer import SparseTrainer

# ä½¿ç”¨SparseTrainerè€Œä¸æ˜¯æ ‡å‡†Trainer
trainer = SparseTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    ...
)

trainer.train()
```

### Denseå¾®è°ƒç‰¹ç‚¹

âœ… **æ›´æ–°æ‰€æœ‰éé›¶å‚æ•°**: å¯èƒ½è·å¾—æ›´å¥½çš„æ€§èƒ½  
âœ… **ä¿æŒç¨€ç–æ€§**: é€šè¿‡æ¢¯åº¦maskç¡®ä¿ç¨€ç–æ¨¡å¼ä¸å˜  
âŒ **æ˜¾å­˜éœ€æ±‚é«˜**: éœ€è¦å­˜å‚¨æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦  
âŒ **è®­ç»ƒæ…¢**: éœ€è¦æ›´æ–°50%çš„å‚æ•°  

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

æ ¹æ®Wandaè®ºæ–‡çš„å®éªŒç»“æœ:

### Llama-7b, 50%ç¨€ç–åº¦

| æ–¹æ³• | WikiText PPL | è®­ç»ƒæ—¶é—´ | æ˜¾å­˜éœ€æ±‚ |
|------|-------------|---------|---------|
| å‰ªæå(æ— å¾®è°ƒ) | 6.42 | - | - |
| + LoRAå¾®è°ƒ | ~5.8 | 12å°æ—¶ | ~20GB |
| + Denseå¾®è°ƒ | ~5.6 | 48å°æ—¶ | ~40GB |
| DenseåŸºçº¿ | 5.12 | - | - |

**ç»“è®º**: LoRAå¾®è°ƒå¯ä»¥æ¢å¤å¤§éƒ¨åˆ†æ€§èƒ½,ä¸”æ•ˆç‡æ›´é«˜ã€‚

## ğŸš€ æ¨èæµç¨‹

### å¿«é€Ÿæ¢å¤ç²¾åº¦(æ¨è)

```bash
# 1. å‰ªæ
python main.py --model ... --prune_method wanda --sparsity_ratio 0.5 ...

# 2. LoRAå¾®è°ƒ
cd lora_ft
python finetune_lm.py --model_name_or_path <pruned_model> ...

# 3. è¯„ä¼°
python evaluate_ppl.py --model <pruned_model> --lora_weights <lora_weights>
```

### è¿½æ±‚æè‡´æ€§èƒ½

```bash
# 1. å‰ªæ
python main.py --model ... --prune_method wanda --sparsity_ratio 0.5 ...

# 2. Denseå¾®è°ƒ(éœ€è¦è‡ªå·±å®ç°è®­ç»ƒè„šæœ¬)
# ä½¿ç”¨ dense_ft/sparse_trainer.py ä¸­çš„ SparseTrainer

# 3. è¯„ä¼°
# ä½¿ç”¨æ ‡å‡†è¯„ä¼°æ–¹æ³•
```

## ğŸ’¡ å…³é”®è¦ç‚¹æ€»ç»“

### LoRAå¾®è°ƒ vs Denseå¾®è°ƒ

**LoRAå¾®è°ƒ**:
- âœ… **ä¸æ˜¯é‡æ–°è®­ç»ƒ**: åªè®­ç»ƒæ–°å¢çš„ä½ç§©çŸ©é˜µ
- âœ… **ä¸æ˜¯å…¨é‡å¾®è°ƒ**: åŸå§‹æƒé‡å†»ç»“
- âœ… **ä¿æŒç¨€ç–æ€§**: å‰ªæçš„æƒé‡æ°¸è¿œä¸º0
- âœ… **å‚æ•°é«˜æ•ˆ**: åªè®­ç»ƒ<1%å‚æ•°

**Denseå¾®è°ƒ**:
- âœ… **ä¸æ˜¯é‡æ–°è®­ç»ƒ**: åŸºäºå‰ªæåçš„æ¨¡å‹ç»§ç»­è®­ç»ƒ
- âœ… **æ˜¯å…¨é‡å¾®è°ƒ**: æ›´æ–°æ‰€æœ‰éé›¶æƒé‡
- âœ… **ä¿æŒç¨€ç–æ€§**: é€šè¿‡æ¢¯åº¦maskç¡®ä¿
- âŒ **å‚æ•°å¯†é›†**: è®­ç»ƒ50%å‚æ•°

### ä¸¤è€…å…±åŒç‚¹

1. **éƒ½ä¸æ˜¯ä»å¤´è®­ç»ƒ**: éƒ½åŸºäºå‰ªæåçš„æ¨¡å‹
2. **éƒ½ä¿æŒç¨€ç–æ€§**: å‰ªæçš„æƒé‡å§‹ç»ˆä¸º0
3. **éƒ½æ˜¯å¾®è°ƒ**: åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šç»§ç»­è®­ç»ƒ
4. **éƒ½èƒ½æ¢å¤æ€§èƒ½**: å¯ä»¥éƒ¨åˆ†æˆ–å®Œå…¨æ¢å¤å‰ªææŸå¤±çš„ç²¾åº¦

## ğŸ“ å®è·µå»ºè®®

1. **é¦–é€‰LoRA**: é™¤éæœ‰å……è¶³çš„è®¡ç®—èµ„æºå’Œæ—¶é—´
2. **æ•°æ®é›†é€‰æ‹©**: C4æ•°æ®é›†æ•ˆæœå¥½,WikiTextä¹Ÿå¯ä»¥
3. **è®­ç»ƒæ ·æœ¬æ•°**: 30000ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ªå¥½çš„èµ·ç‚¹
4. **å­¦ä¹ ç‡**: LoRAä½¿ç”¨1e-4,Denseå¾®è°ƒå¯èƒ½éœ€è¦æ›´å°çš„å­¦ä¹ ç‡
5. **ç›‘æ§ç¨€ç–åº¦**: è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸæ£€æŸ¥ç¨€ç–åº¦æ˜¯å¦ä¿æŒ

## ğŸ”§ åˆ›å»ºLoRAå¾®è°ƒè„šæœ¬

æˆ‘å°†ä¸ºä½ åˆ›å»ºä¸€ä¸ªå¯ä»¥ç›´æ¥ä½¿ç”¨çš„LoRAå¾®è°ƒè„šæœ¬...

