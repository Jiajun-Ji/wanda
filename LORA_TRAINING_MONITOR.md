# LoRAè®­ç»ƒå®æ—¶ç›‘æ§æŒ‡å—

## ğŸ“Š å®æ—¶è¾“å‡ºè¯´æ˜

LoRAè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæœ‰**è¯¦ç»†çš„å®æ—¶è¾“å‡º**ï¼Œå¸®åŠ©ä½ ç›‘æ§è®­ç»ƒè¿›åº¦å’Œæ•ˆæœã€‚

## ğŸš€ å®Œæ•´è¾“å‡ºç¤ºä¾‹

### **é˜¶æ®µ1: åˆå§‹åŒ–ï¼ˆ0-2åˆ†é’Ÿï¼‰**

```bash
========================================
LoRA Fine-tuning Configuration
========================================
Pruned model: out/llama2_7b/block_16x16/wanda/pruned_model
Config: meta-llama/Llama-2-7b-hf
Output directory: out/llama2_7b/block_16x16/wanda/lora_weights_wikitext
Dataset: wikitext
Training samples: 30000
Learning rate: 1e-4
Block size: 1024
Epochs: 1
========================================

âœ… Pruned model found

ğŸš€ Starting LoRA fine-tuning...
Expected training time: ~1-2 hours

# åŠ è½½æ¨¡å‹
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00]
âœ… Model loaded successfully

# åº”ç”¨LoRA
Applying LoRA configuration...
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.062%
âœ… LoRA applied: only 0.062% parameters are trainable

# åŠ è½½æ•°æ®é›†
Loading dataset 'wikitext' (wikitext-2-raw-v1)...
Found cached dataset wikitext
âœ… Dataset loaded
  - Train samples: 2,000 sequences
  - Eval samples: 128 sequences
```

### **é˜¶æ®µ2: è®­ç»ƒè¿‡ç¨‹ï¼ˆ1-2å°æ—¶ï¼‰**

```bash
***** Running training *****
  Num examples = 2000
  Num Epochs = 1
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 1
  Gradient Accumulation steps = 1
  Total optimization steps = 2000
  Number of trainable parameters = 4,194,304

# æ¯10æ­¥è¾“å‡ºä¸€æ¬¡è®­ç»ƒloss
{'loss': 3.4521, 'learning_rate': 1e-04, 'epoch': 0.005}
Step 10/2000   [>                                ] 0.5%  | Loss: 3.4521 | Time: 0:00:30

{'loss': 3.2134, 'learning_rate': 1e-04, 'epoch': 0.01}
Step 20/2000   [>                                ] 1.0%  | Loss: 3.2134 | Time: 0:01:00

{'loss': 2.9876, 'learning_rate': 1e-04, 'epoch': 0.025}
Step 50/2000   [=>                               ] 2.5%  | Loss: 2.9876 | Time: 0:02:30

{'loss': 2.6543, 'learning_rate': 1e-04, 'epoch': 0.05}
Step 100/2000  [===>                             ] 5.0%  | Loss: 2.6543 | Time: 0:05:00

# æ¯200æ­¥è¿›è¡Œä¸€æ¬¡è¯„ä¼°
***** Running Evaluation *****
  Num examples = 128
  Batch size = 8

Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:30<00:00]

{'eval_loss': 2.3456, 'eval_runtime': 30.12, 'eval_samples_per_second': 4.25, 
 'eval_steps_per_second': 0.53, 'epoch': 0.1}

Eval at step 200:
  - Eval Loss: 2.3456
  - Perplexity: 10.44
  - Best so far: Yes âœ…

{'loss': 2.2345, 'learning_rate': 1e-04, 'epoch': 0.1}
Step 200/2000  [======>                          ] 10.0% | Loss: 2.2345 | Time: 0:10:00

# ç»§ç»­è®­ç»ƒ...
Step 300/2000  [=========>                       ] 15.0% | Loss: 2.0123 | Time: 0:15:00

# ç¬¬äºŒæ¬¡è¯„ä¼°
***** Running Evaluation *****
Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:30<00:00]

{'eval_loss': 2.1234, 'eval_runtime': 30.45, 'eval_samples_per_second': 4.20,
 'eval_steps_per_second': 0.52, 'epoch': 0.2}

Eval at step 400:
  - Eval Loss: 2.1234
  - Perplexity: 8.36
  - Best so far: Yes âœ… (improved from 10.44)

Step 400/2000  [============>                    ] 20.0% | Loss: 1.8765 | Time: 0:20:00

# æ¯500æ­¥ä¿å­˜checkpoint
Saving model checkpoint to out/.../lora_weights_wikitext/checkpoint-500
Configuration saved
Model weights saved
âœ… Checkpoint saved at step 500

Step 500/2000  [===============>                 ] 25.0% | Loss: 1.7654 | Time: 0:25:00

# ç»§ç»­è®­ç»ƒ...
Step 1000/2000 [==============================>  ] 50.0% | Loss: 1.5432 | Time: 0:50:00
Step 1500/2000 [============================================>] 75.0% | Loss: 1.4321 | Time: 1:15:00
Step 2000/2000 [=================================================] 100% | Loss: 1.3876 | Time: 1:40:00

Training completed in 1:40:23
```

### **é˜¶æ®µ3: æœ€ç»ˆè¯„ä¼°ï¼ˆ1-2åˆ†é’Ÿï¼‰**

```bash
***** Running Final Evaluation *****
  Num examples = 128
  Batch size = 8

Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:30<00:00]

***** Final eval results *****
  eval_loss = 1.9876
  eval_runtime = 30.67
  eval_samples_per_second = 4.17
  eval_steps_per_second = 0.52
  perplexity = 7.30
  epoch = 1.0

âœ… Final Perplexity: 7.30
   (Improved from 8207.52 â†’ 7.30, reduction: 99.91%)
```

### **é˜¶æ®µ4: ä¿å­˜æ¨¡å‹ï¼ˆ10-30ç§’ï¼‰**

```bash
Saving model checkpoint to out/llama2_7b/block_16x16/wanda/lora_weights_wikitext
Configuration saved in .../adapter_config.json
Model weights saved in .../adapter_model.bin
Tokenizer saved in .../tokenizer_config.json

***** Training summary *****
  Total steps: 2000
  Total time: 1:40:23
  Final train loss: 1.3876
  Final eval loss: 1.9876
  Final perplexity: 7.30
  Best checkpoint: checkpoint-1800

========================================
LoRA Fine-tuning Complete!
========================================
LoRA weights saved to: out/llama2_7b/block_16x16/wanda/lora_weights_wikitext

Next steps:
1. Evaluate the fine-tuned model:
   ./evaluate_lora_block.sh
========================================
```

---

## ğŸ“ˆ å…³é”®æŒ‡æ ‡è§£è¯»

### **1. Training Lossï¼ˆè®­ç»ƒæŸå¤±ï¼‰**

```bash
Step 10   | Loss: 3.4521  # åˆæœŸè¾ƒé«˜
Step 100  | Loss: 2.6543  # å¿«é€Ÿä¸‹é™
Step 500  | Loss: 1.7654  # ç»§ç»­ä¸‹é™
Step 1000 | Loss: 1.5432  # é€æ¸æ”¶æ•›
Step 2000 | Loss: 1.3876  # æœ€ç»ˆæ”¶æ•›
```

**åˆ¤æ–­æ ‡å‡†**ï¼š
- âœ… **æŒç»­ä¸‹é™**: è®­ç»ƒæ­£å¸¸ï¼Œæ¨¡å‹åœ¨å­¦ä¹ 
- âš ï¸ **å‰§çƒˆæ³¢åŠ¨**: å­¦ä¹ ç‡å¯èƒ½è¿‡é«˜
- âš ï¸ **ä¸‹é™ç¼“æ…¢**: å­¦ä¹ ç‡å¯èƒ½è¿‡ä½
- âŒ **ä¸ä¸‹é™**: æ•°æ®æˆ–é…ç½®æœ‰é—®é¢˜

### **2. Eval Lossï¼ˆè¯„ä¼°æŸå¤±ï¼‰**

```bash
Step 200  | Eval Loss: 2.3456 | PPL: 10.44
Step 400  | Eval Loss: 2.1234 | PPL: 8.36  âœ… æ”¹å–„
Step 600  | Eval Loss: 2.0123 | PPL: 7.48  âœ… ç»§ç»­æ”¹å–„
```

**åˆ¤æ–­æ ‡å‡†**ï¼š
- âœ… **æŒç»­ä¸‹é™**: æ³›åŒ–èƒ½åŠ›æå‡
- âš ï¸ **ä¸Šå‡**: å¯èƒ½è¿‡æ‹Ÿåˆ
- âš ï¸ **è¿œé«˜äºtrain loss**: è¿‡æ‹Ÿåˆ

### **3. Perplexityï¼ˆå›°æƒ‘åº¦ï¼‰**

```bash
Initial (block pruned): 8207.52  âŒ
After LoRA (step 200):  10.44    âœ… å¤§å¹…æ”¹å–„
After LoRA (step 400):  8.36     âœ… ç»§ç»­æ”¹å–„
Final (step 2000):      7.30     âœ… æ¥è¿‘ç›®æ ‡
```

**ç›®æ ‡**ï¼š
- ğŸ¯ **ç†æƒ³**: 6.5-7.0
- âœ… **è‰¯å¥½**: 7.0-10.0
- âš ï¸ **ä¸€èˆ¬**: 10.0-50.0
- âŒ **è¾ƒå·®**: >50.0

---

## ğŸ” å®æ—¶ç›‘æ§æŠ€å·§

### **1. ä½¿ç”¨ `tee` ä¿å­˜æ—¥å¿—**

```bash
./run_lora_finetune_block.sh 2>&1 | tee lora_training.log
```

è¿™æ ·å¯ä»¥ï¼š
- å®æ—¶æŸ¥çœ‹è¾“å‡º
- åŒæ—¶ä¿å­˜åˆ°æ–‡ä»¶
- äº‹ååˆ†æè®­ç»ƒè¿‡ç¨‹

### **2. ä½¿ç”¨ `watch` ç›‘æ§GPU**

åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œï¼š
```bash
watch -n 1 nvidia-smi
```

ç›‘æ§ï¼š
- GPUåˆ©ç”¨ç‡ï¼ˆåº”è¯¥æ¥è¿‘100%ï¼‰
- æ˜¾å­˜ä½¿ç”¨ï¼ˆåº”è¯¥ç¨³å®šï¼‰
- æ¸©åº¦ï¼ˆä¸åº”è¿‡é«˜ï¼‰

### **3. ä½¿ç”¨ `tail` å®æ—¶æŸ¥çœ‹æ—¥å¿—**

å¦‚æœåœ¨åå°è¿è¡Œï¼š
```bash
tail -f lora_training.log
```

### **4. ç»˜åˆ¶è®­ç»ƒæ›²çº¿**

è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥ä»æ—¥å¿—ä¸­æå–lossï¼š
```bash
grep "{'loss':" lora_training.log > losses.txt
```

---

## âš ï¸ å¼‚å¸¸æƒ…å†µå¤„ç†

### **æƒ…å†µ1: Lossä¸ä¸‹é™**

```bash
Step 100 | Loss: 3.4521
Step 200 | Loss: 3.4523
Step 300 | Loss: 3.4519
```

**å¯èƒ½åŸå› **ï¼š
- å­¦ä¹ ç‡è¿‡ä½
- æ•°æ®åŠ è½½æœ‰é—®é¢˜
- æ¨¡å‹å†»ç»“è®¾ç½®é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å¢åŠ å­¦ä¹ ç‡
LEARNING_RATE=5e-4  # ä»1e-4å¢åŠ åˆ°5e-4
```

### **æƒ…å†µ2: Losså‰§çƒˆæ³¢åŠ¨**

```bash
Step 100 | Loss: 2.5
Step 110 | Loss: 1.8
Step 120 | Loss: 3.2
Step 130 | Loss: 2.1
```

**å¯èƒ½åŸå› **ï¼š
- å­¦ä¹ ç‡è¿‡é«˜
- Batch sizeå¤ªå°

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# é™ä½å­¦ä¹ ç‡
LEARNING_RATE=5e-5  # ä»1e-4é™ä½åˆ°5e-5

# æˆ–å¢åŠ æ¢¯åº¦ç´¯ç§¯
gradient_accumulation_steps=4
```

### **æƒ…å†µ3: Eval Lossä¸Šå‡**

```bash
Step 200 | Train: 2.0 | Eval: 2.3
Step 400 | Train: 1.5 | Eval: 2.5  âš ï¸ ä¸Šå‡
Step 600 | Train: 1.2 | Eval: 2.8  âŒ ç»§ç»­ä¸Šå‡
```

**å¯èƒ½åŸå› **ï¼š
- è¿‡æ‹Ÿåˆ

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å¢åŠ dropout
lora_dropout=0.1  # ä»0.05å¢åŠ åˆ°0.1

# æˆ–å‡å°‘è®­ç»ƒè½®æ•°
NUM_EPOCHS=1  # ä¸è¦è®­ç»ƒå¤ªå¤šè½®
```

### **æƒ…å†µ4: OOMï¼ˆæ˜¾å­˜ä¸è¶³ï¼‰**

```bash
RuntimeError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å‡å°ä¸Šä¸‹æ–‡é•¿åº¦
BLOCK_SIZE=512  # ä»1024å‡åˆ°512

# æˆ–ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
--gradient_checkpointing
```

---

## ğŸ“Š è¾“å‡ºå‚æ•°è¯´æ˜

### **å·²æ·»åŠ çš„ç›‘æ§å‚æ•°**

```bash
--logging_steps 10              # æ¯10æ­¥è¾“å‡ºä¸€æ¬¡è®­ç»ƒloss
--eval_steps 200                # æ¯200æ­¥è¯„ä¼°ä¸€æ¬¡
--save_steps 500                # æ¯500æ­¥ä¿å­˜checkpoint
--logging_first_step            # è¾“å‡ºç¬¬ä¸€æ­¥çš„loss
--evaluation_strategy steps     # æŒ‰æ­¥æ•°è¯„ä¼°
--save_strategy steps           # æŒ‰æ­¥æ•°ä¿å­˜
--load_best_model_at_end        # åŠ è½½æœ€ä½³æ¨¡å‹
--metric_for_best_model eval_loss  # ä½¿ç”¨eval_lossé€‰æ‹©æœ€ä½³æ¨¡å‹
```

### **è‡ªå®šä¹‰ç›‘æ§é¢‘ç‡**

å¦‚æœæƒ³è¦æ›´é¢‘ç¹çš„è¾“å‡ºï¼š
```bash
--logging_steps 5    # æ¯5æ­¥è¾“å‡º
--eval_steps 100     # æ¯100æ­¥è¯„ä¼°
```

å¦‚æœæƒ³è¦å‡å°‘è¾“å‡ºï¼š
```bash
--logging_steps 50   # æ¯50æ­¥è¾“å‡º
--eval_steps 500     # æ¯500æ­¥è¯„ä¼°
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

1. **ä½¿ç”¨ `tee` ä¿å­˜æ—¥å¿—**: æ–¹ä¾¿äº‹ååˆ†æ
2. **ç›‘æ§GPUä½¿ç”¨**: ç¡®ä¿èµ„æºå……åˆ†åˆ©ç”¨
3. **è§‚å¯Ÿlossè¶‹åŠ¿**: åˆ¤æ–­è®­ç»ƒæ˜¯å¦æ­£å¸¸
4. **å®šæœŸæŸ¥çœ‹eval**: é¿å…è¿‡æ‹Ÿåˆ
5. **ä¿å­˜checkpoint**: é˜²æ­¢è®­ç»ƒä¸­æ–­

---

## ğŸ¯ é¢„æœŸæ—¶é—´çº¿ï¼ˆWikiTextï¼‰

```
0:00:00 - 0:02:00   åˆå§‹åŒ–ï¼ˆåŠ è½½æ¨¡å‹ã€æ•°æ®ï¼‰
0:02:00 - 0:05:00   å‰100æ­¥ï¼ˆlosså¿«é€Ÿä¸‹é™ï¼‰
0:05:00 - 0:20:00   100-400æ­¥ï¼ˆç¬¬ä¸€æ¬¡è¯„ä¼°ï¼‰
0:20:00 - 0:50:00   400-1000æ­¥ï¼ˆä¸­æœŸè®­ç»ƒï¼‰
0:50:00 - 1:20:00   1000-1800æ­¥ï¼ˆåæœŸæ”¶æ•›ï¼‰
1:20:00 - 1:40:00   1800-2000æ­¥ï¼ˆæœ€ç»ˆæ”¶æ•›ï¼‰
1:40:00 - 1:42:00   æœ€ç»ˆè¯„ä¼°å’Œä¿å­˜

æ€»è®¡: ~1å°æ—¶40åˆ†é’Ÿ
```

---

## ğŸ“ æ€»ç»“

LoRAè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæœ‰**éå¸¸è¯¦ç»†çš„å®æ—¶è¾“å‡º**ï¼ŒåŒ…æ‹¬ï¼š

âœ… **è®­ç»ƒè¿›åº¦**: æ¯10æ­¥è¾“å‡ºloss  
âœ… **è¯„ä¼°ç»“æœ**: æ¯200æ­¥è¯„ä¼°perplexity  
âœ… **Checkpoint**: æ¯500æ­¥ä¿å­˜æ¨¡å‹  
âœ… **æœ€ç»ˆç»“æœ**: è®­ç»ƒç»“æŸåçš„å®Œæ•´è¯„ä¼°  

ä½ å¯ä»¥é€šè¿‡è¿™äº›è¾“å‡ºå®æ—¶ç›‘æ§è®­ç»ƒæ•ˆæœï¼ŒåŠæ—¶å‘ç°å’Œè§£å†³é—®é¢˜ï¼

