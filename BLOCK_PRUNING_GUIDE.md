# Block-wise Wanda Pruning Guide

## æ¦‚è¿°

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨16x16å—ç»“æ„åŒ–å‰ªææ–¹æ³•å¯¹Llama-2-7bæ¨¡å‹è¿›è¡Œå‰ªæã€‚

## ä»€ä¹ˆæ˜¯å—å‰ªæï¼Ÿ

### éç»“æ„åŒ–å‰ªæ vs å—å‰ªæ

| ç‰¹æ€§ | éç»“æ„åŒ–å‰ªæ | å—å‰ªæ (16x16) |
|------|-------------|---------------|
| **å‰ªæç²’åº¦** | å•ä¸ªæƒé‡ | 16x16æƒé‡å— |
| **ç¨€ç–æ¨¡å¼** | éšæœºåˆ†æ•£ | å—çŠ¶ç»“æ„åŒ– |
| **ç¡¬ä»¶åŠ é€Ÿ** | å›°éš¾ | å®¹æ˜“ |
| **ç²¾åº¦** | æ›´é«˜ | ç•¥ä½ |
| **å®ç”¨æ€§** | ç ”ç©¶ | éƒ¨ç½² |

### ç®—æ³•æµç¨‹

```
1. è®¡ç®—Wandaåˆ†æ•°çŸ©é˜µ: Score = |W| Ã— âˆš(activation)
2. å°†æƒé‡çŸ©é˜µåˆ’åˆ†ä¸º16x16çš„å—
3. è®¡ç®—æ¯ä¸ªå—çš„å¹³å‡åˆ†æ•°
4. æ ¹æ®ç¨€ç–åº¦ï¼Œä¿ç•™åˆ†æ•°æœ€é«˜çš„å—
5. å°†åˆ†æ•°æœ€ä½çš„å—å…¨éƒ¨ç½®é›¶
```

## ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1ï¼šä½¿ç”¨è¿è¡Œè„šæœ¬ï¼ˆæ¨èï¼‰

```bash
cd /home/jjji/Research/Hybird-Kernel/wanda
./run_prune_llama2_7b_block.sh
```

### æ–¹æ³•2ï¼šç›´æ¥ä½¿ç”¨Pythonè„šæœ¬

```bash
python main_block.py \
    --model /mnt/sdb/llm_models/Llama-2-7b-hf \
    --sparsity_ratio 0.5 \
    --block_size 16 \
    --nsamples 128 \
    --seed 0 \
    --save out/llama2_7b/block_16x16/wanda \
    --save_model out/llama2_7b/block_16x16/wanda/pruned_model
```

### å‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `--model` | æ¨¡å‹è·¯å¾„ | å¿…éœ€ |
| `--sparsity_ratio` | ç›®æ ‡ç¨€ç–åº¦ | 0.5 (50%) |
| `--block_size` | å—å¤§å° | 16 |
| `--nsamples` | æ ¡å‡†æ ·æœ¬æ•° | 128 |
| `--seed` | éšæœºç§å­ | 0 |
| `--save` | ç»“æœä¿å­˜è·¯å¾„ | None |
| `--save_model` | æ¨¡å‹ä¿å­˜è·¯å¾„ | None |

## è¯„ä¼°æ–¹æ³•

### è¯„ä¼°å•ä¸ªæ¨¡å‹

```bash
python eval_additional_pruned.py \
    --model out/llama2_7b/block_16x16/wanda/pruned_model
```

### å¯¹æ¯”éç»“æ„åŒ– vs å—å‰ªæ

```bash
python eval_compare_pruning_types.py \
    --unstructured_model out/llama2_7b/unstructured/wanda/pruned_model \
    --block_model out/llama2_7b/block_16x16/wanda/pruned_model
```

## è¾“å‡ºç¤ºä¾‹

### å‰ªæè¿‡ç¨‹è¾“å‡º

```
================================================================================
Wanda Block Pruning (Block Size: 16x16)
================================================================================
Target sparsity: 50.00%
Block size: 16x16
================================================================================

Loading calibration data (WikiText2)...
Dataset loading complete

================================================================================
Pruning Layer 0
================================================================================
  self_attn.q_proj:
    - Shape: [4096, 4096]
    - Total params: 16,777,216
    - Pruned params: 8,388,608
    - Actual sparsity: 50.0000%
    - Target sparsity: 50.00%
  ...
```

### è¯„ä¼°è¾“å‡º

```
ğŸ“Š COMPARISON RESULTS
================================================================================
Pruning Type                   Sparsity        Perplexity      PPL Diff
--------------------------------------------------------------------------------
Unstructured Wanda (50%)       50.0000%        6.3100          0.0000
Block 16x16 Wanda (50%)        50.0000%        6.5200          +0.2100
================================================================================

ğŸ’¡ ANALYSIS
================================================================================
1. Perplexity Comparison:
   - Unstructured: 6.3100
   - Block 16x16: 6.5200
   - Difference: +0.2100 (+3.33%)

2. Trade-off Analysis:
   âœ… Excellent: Block pruning achieves similar performance
   â†’ Block-structured sparsity is viable for deployment
```

## å®éªŒå»ºè®®

### 1. ä¸åŒå—å¤§å°å¯¹æ¯”

```bash
# 8x8 å—
python main_block.py --block_size 8 --save out/llama2_7b/block_8x8/wanda

# 16x16 å—ï¼ˆæ¨èï¼‰
python main_block.py --block_size 16 --save out/llama2_7b/block_16x16/wanda

# 32x32 å—
python main_block.py --block_size 32 --save out/llama2_7b/block_32x32/wanda
```

### 2. ä¸åŒç¨€ç–åº¦å¯¹æ¯”

```bash
# 30% ç¨€ç–åº¦
python main_block.py --sparsity_ratio 0.3 --save out/llama2_7b/block_16x16/wanda_30

# 50% ç¨€ç–åº¦
python main_block.py --sparsity_ratio 0.5 --save out/llama2_7b/block_16x16/wanda_50

# 70% ç¨€ç–åº¦
python main_block.py --sparsity_ratio 0.7 --save out/llama2_7b/block_16x16/wanda_70
```

## é¢„æœŸç»“æœ

### æ€§èƒ½é¢„æœŸ

| æ¨¡å‹ | ç¨€ç–åº¦ | å›°æƒ‘åº¦ | ç›¸å¯¹å˜åŒ– |
|------|--------|--------|----------|
| Dense | 0% | 5.12 | - |
| Unstructured 50% | 50% | 6.31 | +23% |
| Block 16x16 50% | 50% | 6.5-7.0 | +27-37% |

### åŠ é€Ÿé¢„æœŸ

- **éç»“æ„åŒ–å‰ªæ**ï¼šç†è®ºåŠ é€Ÿ2xï¼Œå®é™…åŠ é€Ÿ1.0-1.2xï¼ˆç¡¬ä»¶æ”¯æŒå·®ï¼‰
- **å—å‰ªæ**ï¼šç†è®ºåŠ é€Ÿ2xï¼Œå®é™…åŠ é€Ÿ1.5-2.0xï¼ˆç¡¬ä»¶å‹å¥½ï¼‰

## æŠ€æœ¯ç»†èŠ‚

### å—åˆ†æ•°è®¡ç®—

```python
# å¯¹æ¯ä¸ª16x16å—è®¡ç®—å¹³å‡åˆ†æ•°
block_score = block.mean()  # ä½¿ç”¨å¹³å‡å€¼ï¼Œä¸å—å—å¤§å°å½±å“
```

### è¾¹ç•Œå¤„ç†

- å¦‚æœæƒé‡çŸ©é˜µç»´åº¦ä¸æ˜¯16çš„å€æ•°ï¼Œè¾¹ç•Œå—ä¼šå°äº16x16
- ä½¿ç”¨å¹³å‡å€¼è€Œéæ€»å’Œï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒ

### ç¨€ç–åº¦å®šä¹‰

- æŒ‰å—æ•°é‡è®¡ç®—ç¨€ç–åº¦ï¼šå‰ªæ‰50%çš„å—
- å®é™…æƒé‡ç¨€ç–åº¦æ¥è¿‘50%ï¼ˆå–å†³äºè¾¹ç•Œå—ï¼‰

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆé€‰æ‹©16x16ï¼Ÿ

A: 16x16æ˜¯GPUç¡¬ä»¶åŠ é€Ÿçš„å¸¸è§å—å¤§å°ï¼Œå¹³è¡¡äº†ç²¾åº¦å’ŒåŠ é€Ÿæ•ˆæœã€‚

### Q2: å—å‰ªææ¯”éç»“æ„åŒ–å‰ªæå·®å¤šå°‘ï¼Ÿ

A: é€šå¸¸å›°æƒ‘åº¦å¢åŠ 3-10%ï¼Œä½†æ¨ç†é€Ÿåº¦å¯æå‡50-100%ã€‚

### Q3: å¯ä»¥ç”¨äºå…¶ä»–æ¨¡å‹å—ï¼Ÿ

A: å¯ä»¥ï¼Œæ”¯æŒæ‰€æœ‰Llamaç³»åˆ—æ¨¡å‹ï¼ˆ7B, 13B, 30B, 65Bï¼‰ã€‚

### Q4: å¦‚ä½•é€‰æ‹©å—å¤§å°ï¼Ÿ

A: 
- 8x8: æ›´ç²¾ç»†ï¼Œç²¾åº¦æ›´é«˜ï¼ŒåŠ é€Ÿæ•ˆæœç•¥å·®
- 16x16: æ¨èï¼Œå¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦
- 32x32: æ›´ç²—ç³™ï¼Œç²¾åº¦è¾ƒä½ï¼ŒåŠ é€Ÿæ•ˆæœæ›´å¥½

## æ–‡ä»¶æ¸…å•

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `lib/prune.py` | æ ¸å¿ƒå‰ªæå‡½æ•°ï¼ˆå·²æ·»åŠ å—å‰ªæï¼‰ |
| `main_block.py` | ä¸»æ‰§è¡Œè„šæœ¬ |
| `run_prune_llama2_7b_block.sh` | è¿è¡Œè„šæœ¬ |
| `eval_compare_pruning_types.py` | å¯¹æ¯”è¯„ä¼°è„šæœ¬ |
| `BLOCK_PRUNING_GUIDE.md` | æœ¬æ–‡æ¡£ |

## å‚è€ƒèµ„æ–™

- Wandaè®ºæ–‡: https://arxiv.org/abs/2306.11695
- Block Sparseè®ºæ–‡: https://arxiv.org/abs/2104.08378
- NVIDIA Block Sparse: https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/

