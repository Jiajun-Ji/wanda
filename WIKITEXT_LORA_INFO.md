# WikiTextæ•°æ®é›†LoRAå¾®è°ƒè¯´æ˜

## ğŸ“Š WikiText vs C4 æ•°æ®é›†å¯¹æ¯”

| ç‰¹æ€§ | WikiText-2 | C4 |
|------|-----------|-----|
| **è®­ç»ƒæ ·æœ¬æ•°** | ~36K tokens (~2-3K sequences) | æ•°ç™¾ä¸‡sequences |
| **æ•°æ®è´¨é‡** | é«˜è´¨é‡ç»´åŸºç™¾ç§‘æ–‡ç«  | ç½‘é¡µçˆ¬å–æ•°æ® |
| **è®­ç»ƒæ—¶é—´** | ~1-2å°æ—¶ | ~12å°æ—¶ |
| **é€‚ç”¨åœºæ™¯** | å¿«é€ŸéªŒè¯ã€å­¦æœ¯ç ”ç©¶ | ç”Ÿäº§éƒ¨ç½² |

## âš ï¸ é‡è¦æç¤º

### WikiTextæ•°æ®é›†è¾ƒå°

WikiText-2è®­ç»ƒé›†åªæœ‰çº¦**2-3Kä¸ªåºåˆ—**ï¼ˆå–å†³äºåˆ†è¯æ–¹å¼ï¼‰ï¼Œè¿œå°äºC4æ•°æ®é›†ã€‚

**å½±å“**ï¼š
- âœ… **è®­ç»ƒå¿«**: 1-2å°æ—¶å³å¯å®Œæˆ
- âš ï¸ **æ ·æœ¬å°‘**: å¯èƒ½æ— æ³•å……åˆ†å¾®è°ƒ
- âš ï¸ **è¿‡æ‹Ÿåˆé£é™©**: å®¹æ˜“è¿‡æ‹Ÿåˆåˆ°WikiText

### å»ºè®®çš„è®­ç»ƒæ ·æœ¬æ•°

```bash
# WikiText-2 å®é™…å¯ç”¨æ ·æœ¬æ•°
# è®­ç»ƒé›†: ~2-3K sequences (block_size=1024)
# éªŒè¯é›†: ~200 sequences

# æ¨èè®¾ç½®
MAX_TRAIN_SAMPLES=2000   # ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®
MAX_EVAL_SAMPLES=128     # éªŒè¯æ ·æœ¬
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1: ä½¿ç”¨æä¾›çš„è„šæœ¬ï¼ˆå·²é…ç½®WikiTextï¼‰

```bash
cd /home/jjji/Research/Hybird-Kernel/wanda

# ç›´æ¥è¿è¡Œï¼ˆå·²é…ç½®ä¸ºWikiTextï¼‰
./run_lora_finetune_block.sh
```

### æ–¹æ³•2: æ‰‹åŠ¨è¿è¡Œ

```bash
cd /home/jjji/Research/Hybird-Kernel/wanda/lora_ft

python finetune_lm.py \
    --model_name_or_path ../out/llama2_7b/block_16x16/wanda/pruned_model \
    --config_name meta-llama/Llama-2-7b-hf \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --num_train_epochs 1 \
    --block_size 1024 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --max_train_samples 2000 \
    --max_eval_samples 128 \
    --learning_rate 1e-4 \
    --overwrite_output_dir \
    --output_dir ../out/llama2_7b/block_16x16/wanda/lora_weights_wikitext
```

## ğŸ“ é…ç½®è¯´æ˜

### å½“å‰é…ç½®

```bash
DATASET="wikitext"
DATASET_CONFIG="wikitext-2-raw-v1"
MAX_TRAIN_SAMPLES=30000  # å®é™…ä¼šè¢«é™åˆ¶ä¸º~2000
OUTPUT_DIR="out/llama2_7b/block_16x16/wanda/lora_weights_wikitext"
```

### æ¨èè°ƒæ•´

ç”±äºWikiTextæ•°æ®é›†è¾ƒå°ï¼Œå»ºè®®ï¼š

```bash
# é€‰é¡¹1: å¤šè½®è®­ç»ƒ
NUM_EPOCHS=3  # å¢åŠ åˆ°3è½®

# é€‰é¡¹2: è°ƒæ•´å­¦ä¹ ç‡
LEARNING_RATE=5e-5  # é™ä½å­¦ä¹ ç‡é¿å…è¿‡æ‹Ÿåˆ

# é€‰é¡¹3: å¢åŠ LoRAç§©
# ä¿®æ”¹ finetune_lm.py ä¸­çš„ lora_r=16
```

## ğŸ¯ é¢„æœŸæ•ˆæœ

### WikiTextå¾®è°ƒé¢„æœŸ

| é˜¶æ®µ | å›°æƒ‘åº¦ | è¯´æ˜ |
|------|--------|------|
| å—å‰ªæå | 8207.52 | âŒ å½“å‰çŠ¶æ€ |
| WikiText LoRA (1 epoch) | 50-100 | âš ï¸ å¯èƒ½ä¸å¤Ÿ |
| WikiText LoRA (3 epochs) | 20-50 | âœ… é¢„æœŸæ”¹å–„ |
| ç†æƒ³ç›®æ ‡ | 6.5-7.0 | ğŸ¯ éœ€è¦æ›´å¤šæ•°æ® |

**æ³¨æ„**: WikiTextæ•°æ®é‡è¾ƒå°ï¼Œå¯èƒ½æ— æ³•å®Œå…¨æ¢å¤åˆ°ç†æƒ³å›°æƒ‘åº¦ã€‚

## ğŸ’¡ å»ºè®®

### å¦‚æœWikiTextæ•ˆæœä¸ä½³

#### é€‰é¡¹1: ä½¿ç”¨C4æ•°æ®é›†ï¼ˆæ¨èï¼‰

```bash
# ä¿®æ”¹ run_lora_finetune_block.sh
DATASET="c4"
# åˆ é™¤ DATASET_CONFIG è¡Œ
MAX_TRAIN_SAMPLES=30000
```

#### é€‰é¡¹2: å¢åŠ è®­ç»ƒè½®æ•°

```bash
NUM_EPOCHS=5  # WikiTextä¸Šè®­ç»ƒ5è½®
```

#### é€‰é¡¹3: æ··åˆæ•°æ®é›†

```python
# éœ€è¦ä¿®æ”¹ finetune_lm.py
# åŒæ—¶ä½¿ç”¨ WikiText + C4
```

## ğŸ“Š æ•°æ®é›†è¯¦ç»†ä¿¡æ¯

### WikiText-2-raw-v1

```
è®­ç»ƒé›†:
- åŸå§‹tokens: ~2,088,628
- Sequences (block_size=1024): ~2,000
- Sequences (block_size=2048): ~1,000

éªŒè¯é›†:
- åŸå§‹tokens: ~217,646
- Sequences (block_size=1024): ~200

æµ‹è¯•é›†:
- åŸå§‹tokens: ~245,569
- Sequences (block_size=1024): ~240
```

### C4 (å¯¹æ¯”)

```
è®­ç»ƒé›†:
- æ•°ç™¾ä¸‡sequences
- å¯ä»¥è®¾ç½®ä»»æ„ max_train_samples

éªŒè¯é›†:
- æ•°åƒsequences
```

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜1: è®­ç»ƒæ ·æœ¬ä¸è¶³è­¦å‘Š

```
Warning: max_train_samples (30000) is larger than dataset size (2000)
Using all available samples: 2000
```

**è§£å†³**: è¿™æ˜¯æ­£å¸¸çš„ï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨å…¨éƒ¨å¯ç”¨æ ·æœ¬ã€‚

### é—®é¢˜2: è¿‡æ‹Ÿåˆ

```
Train Loss: 0.5
Eval Loss: 2.5  # è¿œå¤§äºè®­ç»ƒloss
```

**è§£å†³**:
```bash
# é™ä½å­¦ä¹ ç‡
LEARNING_RATE=5e-5

# å¢åŠ dropout
# ä¿®æ”¹ finetune_lm.py ä¸­çš„ lora_dropout=0.1
```

### é—®é¢˜3: æ•ˆæœä¸ä½³

**è§£å†³**: åˆ‡æ¢åˆ°C4æ•°æ®é›†
```bash
# ä¿®æ”¹ run_lora_finetune_block.sh
DATASET="c4"
MAX_TRAIN_SAMPLES=30000
```

## ğŸ“ˆ ç›‘æ§å»ºè®®

### è®­ç»ƒè¿‡ç¨‹

```bash
# è§‚å¯Ÿè®­ç»ƒlossæ˜¯å¦ä¸‹é™
Step 100/2000 | Train Loss: 3.5
Step 200/2000 | Train Loss: 2.8
Step 500/2000 | Train Loss: 2.1
...

# è§‚å¯Ÿè¯„ä¼°loss
Eval Loss: 2.3
Eval Perplexity: 9.97
```

### åˆ¤æ–­æ ‡å‡†

- âœ… **è®­ç»ƒlossæŒç»­ä¸‹é™**: æ¨¡å‹åœ¨å­¦ä¹ 
- âš ï¸ **è®­ç»ƒlossä¸‹é™ï¼Œeval lossä¸Šå‡**: è¿‡æ‹Ÿåˆ
- âŒ **è®­ç»ƒlossä¸ä¸‹é™**: å­¦ä¹ ç‡è¿‡é«˜æˆ–æ•°æ®é—®é¢˜

## ğŸ“ æ€»ç»“

### WikiTextä¼˜åŠ¿
- âœ… è®­ç»ƒå¿«ï¼ˆ1-2å°æ—¶ï¼‰
- âœ… æ•°æ®è´¨é‡é«˜
- âœ… é€‚åˆå¿«é€ŸéªŒè¯

### WikiTextåŠ£åŠ¿
- âŒ æ•°æ®é‡å°
- âŒ å¯èƒ½æ— æ³•å……åˆ†å¾®è°ƒ
- âŒ å®¹æ˜“è¿‡æ‹Ÿåˆ

### æ¨èç­–ç•¥

1. **å¿«é€ŸéªŒè¯**: å…ˆç”¨WikiTextæµ‹è¯•ï¼ˆ1-2å°æ—¶ï¼‰
2. **æŸ¥çœ‹æ•ˆæœ**: å¦‚æœå›°æƒ‘åº¦é™åˆ°50ä»¥ä¸‹ï¼Œè¯´æ˜æœ‰æ•ˆ
3. **å®Œæ•´è®­ç»ƒ**: åˆ‡æ¢åˆ°C4è¿›è¡Œå®Œæ•´è®­ç»ƒï¼ˆ12å°æ—¶ï¼‰

## ğŸ”— ç›¸å…³æ–‡ä»¶

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `run_lora_finetune_block.sh` | å·²é…ç½®WikiText |
| `evaluate_lora_block.sh` | è¯„ä¼°è„šæœ¬ |
| `LORA_FINETUNE_BLOCK_GUIDE.md` | å®Œæ•´æŒ‡å— |

